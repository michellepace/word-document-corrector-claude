{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michellepace/word-document-corrector-claude/blob/main/word_document_corrector_claude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKva1p6voV3J"
      },
      "source": [
        "# **1. ABOUT**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Corrects large Word documents at a level that surpasses Microsoft Word. Upload any .docx file and see corrections in colour.\n",
        "- **Deep language correction** beyond Microsoft Word capabilities.\n",
        "- **Corrects** grammar, spelling, and inappropriate word choice in colour.\n",
        "- **Large documents** (tested up to 100,000 words, approx. 150 pages).\n",
        "- **Multi-language:** English, German, Italian.\n",
        "- **Preserves** writting style and semantic meaning.\n",
        "- **Comprehensive** testing suite to validate correction integrity.\n",
        "\n",
        "<!-- Figure Clickable 700px Width -->\n",
        "<figure>\n",
        " <figcaption></figcaption>\n",
        "<a href=\"https://michellepace.github.io/word-document-corrector-claude/images/overview.jpg\"\n",
        "target=\"_blank\">\n",
        "  <img src=\"https://michellepace.github.io/word-document-corrector-claude/images/overview.jpg\"\n",
        "  width=\"450\" alt=\"What this notebook does\" />\n",
        "</a>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "9vuvZCaJZEfc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-IDYNNTg8cE"
      },
      "source": [
        "## Example Output\n",
        "\n",
        "View an example of the corrections made:\n",
        "\n",
        "- **Input file:** [MyWordDoc.docx](https://michellepace.github.io/word-document-corrector-claude/example-files/MyWordDoc.docx) - sample Word document with various errors.\n",
        "- **Output file with visual corrections:** [MyWordDoc.docx.PROCESSED.html](https://michellepace.github.io/word-document-corrector-claude/example-files/MyWordDoc.docx.PROCESSED.html)\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq1vbrVA8S8n"
      },
      "source": [
        "## Solution\n",
        "\n",
        "This notebook created with little pre-existing knowledge and in partnership with Claude 3.5 Sonnet, specifically used for:\n",
        "\n",
        "- **Solution design:** Starting with project goals then an hour of brainstorming.\n",
        "- **Technical guidance:** for choosing appropriate Python libraries.\n",
        "- **Code generation:** About 60% of the code was generated by Claude.\n",
        "- **Testing:** Test plan creation and test data for prompt testing.\n",
        "\n",
        "<!-- /Figure with caption 150px width-->\n",
        "<figure>\n",
        " <figcaption>Fig 1: Initial Solution Brainstorming</figcaption>\n",
        " <a href=\"https://michellepace.github.io/word-document-corrector-claude/images/initial-solution-brainstorming.jpg\"\n",
        "    target=\"_blank\">\n",
        "   <img src=\"https://michellepace.github.io/word-document-corrector-claude/images/initial-solution-brainstorming.jpg\"\n",
        "        width=\"150\"\n",
        "        alt=\"Dialogue to Design: Capturing our initial problem-solving conversation\" />\n",
        " </a>\n",
        "</figure>\n",
        "\n",
        "<!-- /Figure with caption 300px width-->\n",
        "<figure>\n",
        "  <figcaption>Fig 2: Initial Solution Plan </figcaption>\n",
        "  <a href=\"https://michellepace.github.io/word-document-corrector-claude/images/initial-solution-sketch.jpg\"\n",
        "     target=\"_blank\">\n",
        "    <img src=\"https://michellepace.github.io/word-document-corrector-claude/images/initial-solution-sketch.jpg\"\n",
        "         width=\"300\"\n",
        "         alt=\"Initial Solution Sketch\" />\n",
        "  </a>\n",
        "</figure>\n",
        "\n",
        "<!-- /Figure with caption 400px width-->\n",
        "<figure>\n",
        "  <figcaption>Fig 3: Final Solution Diagram (generated from notebook code)</figcaption>\n",
        "  <a href=\"https://michellepace.github.io/word-document-corrector-claude/notebook-images/final-solution-diagram.jpg\"\n",
        "     target=\"_blank\">\n",
        "    <img src=\"https://michellepace.github.io/word-document-corrector-claude/images/final-solution-diagram.jpg\"\n",
        "         width=\"400\"\n",
        "         alt=\"Digital Drafting: Claude's visual take on our solution\" />\n",
        "  </a>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK40_oEOhamp"
      },
      "source": [
        "<br>\n",
        "\n",
        "# **2. SETUP**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Libraries"
      ],
      "metadata": {
        "id": "aipU6tgyFspf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8rfyDWRetEbu",
        "outputId": "764ae2e0-482e-4609-f0d5-e7924eceb0cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m222.8/222.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSuccess! All required libraries are installed.\n",
            "Success! Library imports are complete.\n"
          ]
        }
      ],
      "source": [
        "# Install libraries that Colab does not have pre-installed.\n",
        "try:\n",
        "    !pip install --upgrade-strategy only-if-needed --quiet anthropic langchain markdown numpy python-docx scikit-learn sentence-transformers strip-markdown tqdm\n",
        "    print(\"Success! All required libraries are installed.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during library installation: {str(e)}\")\n",
        "\n",
        "# Import all required libraries.\n",
        "try:\n",
        "    from google.colab import drive, files # Request access to your Word Document in Colab\n",
        "    from google.colab import userdata # Request access to your secure Colab Secret: ANTHROPIC_API_KEY\n",
        "    import docx # Read and write Word documents; extract paragraphs and create test Word docs.\n",
        "    from pathlib import Path # Handle files easily and concisely\n",
        "    import anthropic # Interact with Claude Model via Anthropic's API\n",
        "    from langchain.text_splitter import MarkdownTextSplitter # Split markdown text into chunks\n",
        "    from strip_markdown import strip_markdown # Remove markdown for content comparison\n",
        "    from markdown import markdown # Convert corrected markdown file into a pretty HTML page\n",
        "    import re # Analyze text using regular expressions\n",
        "    from collections import Counter # To easily print test results ------------------------------------\n",
        "    from tabulate import tabulate # To easily print test results\n",
        "    import numpy as np # Process numerical data for preservation scores\n",
        "    from tqdm.auto import tqdm # Display progress bar for chunk processing\n",
        "    from sentence_transformers import SentenceTransformer # Transform text chunks into comparable vectors of meaning\n",
        "    from sklearn.metrics.pairwise import cosine_similarity # Calculates semantic similarity between text chunk vectors\n",
        "    print(\"Success! Library imports are complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during library importation: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ4gmzmTwBOF"
      },
      "source": [
        "<a name=\"id-configuration\"></a>\n",
        "\n",
        "## Your Settings\n",
        "\n",
        "**Complete Notebook Usage Steps:**\n",
        "1. Top left corner of Notebook: **File > Save a copy in Drive.**\n",
        "1. Top left corner of Notebook: **Runtime > Run all.**\n",
        "1. Follow the steps in cell below: \"**Set Word Document â­**\"\n",
        "1. Follow the steps in next cell below: \"**Set Secret API key â­**\"\n",
        "1. Top left corner of Notebook: **Runtime > Run cell and below**\n",
        "1. Wait for notebook to finish running all code.\n",
        "1. Find the corrections in the same folder as your Word document.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set Word Document â­ { vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown **Instructions:**\n",
        "\n",
        "#@markdown **Step 1:** Upload the Word document to your [Google Drive](https://drive.google.com/drive/my-drive)<br>\n",
        "#@markdown **Step 2:** Input the full name of the Word document file (see Example below)<br>\n",
        "#@markdown **Step 3:** Run this cell by clicking the little \"play\" icon just under the title<br>\n",
        "\n",
        "#@markdown **Example:**<br>\n",
        "#@markdown - Google Drive files ALWAYS start with: <font color='#FF1493'>/content/â€‹drive/â€‹MyDrive/</font><br>\n",
        "#@markdown - For example: `/content/drive/MyDrive/my-animal-folder/pony.docx`<br><br>\n",
        "\n",
        "my_input_docx_file = '/content/drive/MyDrive/TEST/Word.docx' #@param {type:\"string\", placeholder:\"(here is an example)   /content/drive/MyDrive/MyWordDoc.docx\"}\n",
        "\n",
        "def validate_google_drive_docx(file_path: str) -> Path:\n",
        "    \"\"\"\n",
        "    Validate the input Word document file in Google Drive.\n",
        "\n",
        "    :param file_path: String path to Word document on Google Drive\n",
        "    :return: Path object of the validated file\n",
        "    :raises: Various exceptions for invalid cases\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Check if input is empty\n",
        "        if not file_path.strip():\n",
        "            raise ValueError(\"No file path provided in input box\")\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not Path('/content/drive').exists():\n",
        "            try:\n",
        "                drive.mount('/content/drive')\n",
        "            except Exception as e:\n",
        "                if \"credential propagation was unsuccessful\" in str(e).lower():\n",
        "                    raise PermissionError(\"You denied access to Google Drive.\")\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "        file_path = Path(file_path)\n",
        "\n",
        "        # Check if the file exists\n",
        "        if not file_path.exists():\n",
        "            raise FileNotFoundError(f\"This file doesn't exist in your Google Drive: '{file_path}'\")\n",
        "\n",
        "        # Check if the file has a .docx extension\n",
        "        if file_path.suffix.lower() != '.docx':\n",
        "            raise ValueError(f\"This file doesn't have a .docx extension: '{file_path}'\")\n",
        "\n",
        "        # Check if the file is not empty\n",
        "        if file_path.stat().st_size == 0:\n",
        "            raise ValueError(f\"This file is empty: '{file_path}'\")\n",
        "\n",
        "        # Check if the file can be opened as a Word document\n",
        "        try:\n",
        "            docx.Document(file_path)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"This file couldn't be opened: '{file_path}'. Error: {str(e)}\")\n",
        "\n",
        "        print(\"Success!\")\n",
        "        print(f\"â€¢ Word document found: {file_path.absolute()}\")\n",
        "        print(f\"â€¢ I'll be sending this to Claude for correction\")\n",
        "        return file_path\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"ğŸ›‘ Error: {str(ve)}\")\n",
        "        print(\"To fix:\")\n",
        "        if str(ve) == \"No file path provided in input box\":\n",
        "            print(\" 1. You must input your Word document file name in the input box above.\")\n",
        "            print(\" 2. Read and follow the each step above.\")\n",
        "        else:\n",
        "            print(\" 1. Ensure the file is a valid .docx document\")\n",
        "            print(\" 2. Check if the file is not corrupted or empty\")\n",
        "        print(\"ğŸ›‘ğŸ›‘ğŸ›‘\\n\")\n",
        "        raise\n",
        "    except PermissionError as pe:\n",
        "        print(f\"ğŸ›‘ Error: {str(pe)}\")\n",
        "        print(\"To fix:\")\n",
        "        print(\" 1. Rerun this block and click \\\"Connect to Google Drive\\\"\")\n",
        "        print(\" Worried about safety? Save your own copy of this Notebook and run that.\")\n",
        "        print(\"ğŸ›‘ğŸ›‘ğŸ›‘\\n\")\n",
        "        raise\n",
        "    except FileNotFoundError as fnf:\n",
        "        print(f\"ğŸ›‘ Error: {str(fnf)}\")\n",
        "        print(\"To fix:\")\n",
        "        print(\" 1. In Input Box instructions, look at the pink Example given\")\n",
        "        print(\" 2. Verify your file exists in your Google Drive on that exact path\")\n",
        "        print(\" 3. Remember, file paths and names are case-sensitive\")\n",
        "        print(\"ğŸ›‘ğŸ›‘ğŸ›‘\\n\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(\"ğŸ›‘ Unexpected error occurred\")\n",
        "        print(f\"Error details: {str(e)}\")\n",
        "        print(\"Sorry... really don't know how you got here.\")\n",
        "        print(\"ğŸ›‘ğŸ›‘ğŸ›‘\\n\")\n",
        "        raise\n",
        "\n",
        "\n",
        "### Do the work\n",
        "try:\n",
        "    my_input_docx_file = validate_google_drive_docx(my_input_docx_file)\n",
        "except Exception:\n",
        "    print(\"Please update the file path and run this block again.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "aI0KXrLc1Xg_",
        "outputId": "99bb1041-3906-48cb-d2c6-a7b2719634e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Success!\n",
            "â€¢ Word document found: /content/drive/MyDrive/TEST/Word.docx\n",
            "â€¢ I'll be sending this to Claude for correction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set Secret API key â­ { vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown **Instructions:**<br>\n",
        "\n",
        "#@markdown **Step 1:** Create an [Anthropic API key](https://console.anthropic.com/settings/keys).<br>\n",
        "#@markdown **Step 2:** Then run this cell by clicking the 'play' icon just under the title.<br>\n",
        "#@markdown **Step 3:** you will be guided to setup a Colab secret if you don't have one.<br>\n",
        "\n",
        "#@markdown\n",
        "\n",
        "anthropic_api_secret_name = 'ANTHROPIC_API_KEY'  # @param {type: \"string\"}\n",
        "\n",
        "def test_anthropic_connection(anthropic_client: anthropic.Anthropic) -> None:\n",
        "    my_test_prompt = \"Hello Claude, have I connected to you? (answer briefly!)\"\n",
        "    print()\n",
        "    try:\n",
        "        message = anthropic_client.messages.create(\n",
        "            model=\"claude-3-5-sonnet-latest\",\n",
        "            max_tokens=20,\n",
        "            messages=[{\"role\": \"user\", \"content\": my_test_prompt}]\n",
        "        )\n",
        "        print(\"Success!\\nAPI key is valid and working.\")\n",
        "        print(f\"â€¢ My prompt was:  {my_test_prompt}\")\n",
        "        print(f\"â€¢ Claude responded: {message.content[0].text}\")\n",
        "\n",
        "    except anthropic.APIError as e:\n",
        "        print(f\"API error occurred: {e}\")\n",
        "        raise KeyboardInterrupt(\"Connection test failed. Stopping execution ğŸ›‘.\") from e\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error occurred: {e}\")\n",
        "        raise KeyboardInterrupt(\"Connection test failed. Stopping execution ğŸ›‘.\") from e\n",
        "\n",
        "\n",
        "def validate_anthropic_api_key_format(api_key):\n",
        "    if not api_key.startswith('sk-'):\n",
        "        raise ValueError(\"Anthropic API keys start with \\\"sk-\\\"\")\n",
        "    if ' ' in api_key:\n",
        "        raise ValueError(\"Anthropic API keys don't have white spaces.\")\n",
        "    if len(api_key) <= 100:\n",
        "        raise ValueError(\"Anthropic API keys are longer than 100 characters.\")\n",
        "\n",
        "\n",
        "def get_anthropic_api_key(secret_name):\n",
        "    try:\n",
        "        api_key = userdata.get(secret_name)\n",
        "        validate_anthropic_api_key_format(api_key)\n",
        "        print(\"Success!\")\n",
        "        print(f'Your Colab secret \"{secret_name}\" was found.')\n",
        "        print(f\"â€¢ If the secret contains a valid API Key, we can connect to Claude.\")\n",
        "        print(f'â€¢ To change API Key: Click the \"key\" icon in left handside panel, delete \"{secret_name}\", rerun this block.')\n",
        "        return api_key\n",
        "\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(f\"ğŸ›‘ Error: Colab secret '{secret_name}' not found in your Colab environment\")\n",
        "        print(\" To fix:\")\n",
        "        print(f\" 1. Click the \\\"key\\\" icon on the left of this Notebook\")\n",
        "        print(f\" 2. Add new secret with name '{secret_name}'\")\n",
        "        print(f\" 3. Set value to Anthropic API key from: https://console.anthropic.com/settings/keys\")\n",
        "        print(f\" 4. Rerun this block and follow next instructions\")\n",
        "        print(\" About Colab secrets: https://bit.ly/4cad0v7\")\n",
        "        print(\"ğŸ›‘ğŸ›‘ğŸ›‘\\n\")\n",
        "        raise\n",
        "    except userdata.NotebookAccessError:\n",
        "        print(f\"ğŸ›‘ Error: You denied this Notebook access to your Colab secret '{secret_name}'\")\n",
        "        print(\" To fix:\")\n",
        "        print(\" 1. Rerun this block and click \\\"Grant access\\\"\")\n",
        "        print(\" About Colab secrets: https://bit.ly/4cad0v7\")\n",
        "        print(\" Worried about safety? Save your own copy of this Notebook and run that.\")\n",
        "        print(\"ğŸ›‘ğŸ›‘ğŸ›‘\\n\")\n",
        "        raise\n",
        "    except ValueError as ve:\n",
        "        print(f\"ğŸ›‘ Error: Invalid format, {str(ve)}\")\n",
        "        print(\" To fix:\")\n",
        "        print(f\" 1. Click the \\\"key\\\" icon on the left of this Notebook\")\n",
        "        print(f\" 2. Delete '{anthropic_api_secret_name}'\")\n",
        "        print(f\" 4. Rerun this block and follow next instructions\")\n",
        "        print(\"ğŸ›‘ğŸ›‘ğŸ›‘\\n\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(\"ğŸ›‘ Unexpected error occurred\")\n",
        "        print(\" Please check:\")\n",
        "        print(f\" 1. '{secret_name}' secret exists in Colab (click \\\"key\\\" icon on the left)\")\n",
        "        print(\" 2. Secret value is a valid Anthropic API key\")\n",
        "        print(\" Get API key: https://console.anthropic.com/settings/keys\")\n",
        "        print(\" About Colab secrets: https://bit.ly/4cad0v7\")\n",
        "        print(\"ğŸ›‘ğŸ›‘ğŸ›‘\\n\")\n",
        "        raise\n",
        "\n",
        "\n",
        "### Do the work\n",
        "MY_ANTHROPIC_API_KEY = get_anthropic_api_key(anthropic_api_secret_name)\n",
        "### Do the work\n",
        "\n",
        "anthropic_client = anthropic.Anthropic(\n",
        "    api_key=MY_ANTHROPIC_API_KEY, # From: \"Set Secret API key â­\"\n",
        "    max_retries=2, # Maximum retry attempts\n",
        "    timeout=10, # Timeout of the retry\n",
        ")\n",
        "\n",
        "test_anthropic_connection(anthropic_client)"
      ],
      "metadata": {
        "id": "ozUKbVZZAd8Y",
        "outputId": "400c8052-7b98-414b-c40d-c8a41fdf286f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "Your Colab secret \"ANTHROPIC_API_KEY\" was found.\n",
            "â€¢ If the secret contains a valid API Key, we can connect to Claude.\n",
            "â€¢ To change API Key: Click the \"key\" icon in left handside panel, delete \"ANTHROPIC_API_KEY\", rerun this block.\n",
            "\n",
            "Success!\n",
            "API key is valid and working.\n",
            "â€¢ My prompt was:  Hello Claude, have I connected to you? (answer briefly!)\n",
            "â€¢ Claude responded: Yes, I'm here and ready to chat!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ5GCMvxxVD-"
      },
      "source": [
        "## All Other Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Output files"
      ],
      "metadata": {
        "id": "F4uVrCl0TGYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Files (not AI Settings)\n",
        "original_md_file = Path(f\"{my_input_docx_file}.ORIG.md\") # Word doc extracted into markdown format.\n",
        "processed_md_file = Path(f\"{my_input_docx_file}.PROCESSED.md\") # Word doc with all the corrections, in markdown format.\n",
        "processed_html_file = Path(f\"{my_input_docx_file}.PROCESSED.html\") # Word doc corrections, in HTML format for easy reading."
      ],
      "metadata": {
        "id": "syY3VgO4TJuz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- AI settings (excluding prompt)"
      ],
      "metadata": {
        "id": "KTvoFdf2TM3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking Word Doc. Splitter preserves markdown elements.\n",
        "TARGET_CHUNK_CHARACTERS = 4400 # (approx. 600 words)\n",
        "\n",
        "# Testing Simularity between original/processed chunk\n",
        "SIMILARITY_LARGE_LANGAUGE_MODEL = 'paraphrase-multilingual-mpnet-base-v2'\n",
        "\n",
        "# Everything Claude:\n",
        "CLAUDE_MODEL  = \"claude-3-5-sonnet-latest\"\n",
        "PROMPT_TEMP = 0  # Low temperature for more probable and consistent output (0 to 1)\n",
        "\n",
        "MY_ANTHROPIC_CLIENT = anthropic.Anthropic(\n",
        "    api_key=MY_ANTHROPIC_API_KEY, # From: \"Set Secret API key â­\"\n",
        "    max_retries=2,  # Maximum retry attempts per API request (text chunk)\n",
        "    timeout=20.0   # Timeout in seconds for each individual API request (text chunk)\n",
        ")"
      ],
      "metadata": {
        "id": "HGDF9mDYTSAp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - AI settings (prompt for Claude)"
      ],
      "metadata": {
        "id": "eag2BFBNTWj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = \"\"\"CRITICAL: PROVIDE ONLY THE CORRECTED TEXT WITHOUT ANY ADDITIONAL COMMENTARY.\n",
        "\n",
        "Your task is to take the provided text and rewrite it into a clear, grammatically correct version while preserving the original meaning as closely as possible. Correct any spelling mistakes, punctuation errors, verb tense issues, word choice problems, and other grammatical mistakes.\n",
        "\n",
        "MANDATORY INSTRUCTIONS:\n",
        "\n",
        "1. Determine and use the same linguistic language as the original text (e.g., English, German)\n",
        "2. Preserve all existing markdown formatting, including heading levels, paragraphs, and lists\n",
        "3. Make necessary grammatical corrections, including spelling, punctuation, verb tense, word choice, and other grammatical issues. Only make stylistic changes if essential for clarity\n",
        "4. Mark corrections with markdown syntax, apply one of these choices only:\n",
        "   - For changed text use bold: e.g., **changed** and **multiple changed words**\n",
        "   - For new text use bold: **new words**\n",
        "   - For removed text use bold strikethrough: **~~removed words~~**\n",
        "5. Maintain the original structure:\n",
        "   - Don't add new lines of text\n",
        "   - Don't include additional commentary at all\n",
        "   - Don't convert markdown elements to different types\n",
        "6. For ambiguous corrections, choose the option that best preserves original meaning and style\n",
        "7. Ensure consistency in corrections throughout the text\n",
        "8. Return the corrected text in markdown syntax\n",
        "9. DO NOT add any explanations, introductions, or conclusions to your response\n",
        "\n",
        "FINAL REMINDER: Your output should consist SOLELY of the corrected text. Do not include phrases like \"Here is the corrected text\" or any other form of commentary.\n",
        "\n",
        "The text to be corrected is provided between the triple tildes (~~~):\n",
        "\n",
        "~~~\n",
        "{the_markdown_chunk}\n",
        "~~~\n",
        "\n",
        "REMEMBER: Provide ONLY the corrected text without any additional words or explanations.\"\"\""
      ],
      "metadata": {
        "id": "2lJ16ASB0mbd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Success! All settings configured.\")"
      ],
      "metadata": {
        "id": "AhEauatfTg3F",
        "outputId": "81c07cd3-3f7e-4257-f786-83b380cfd251",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! All settings configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "lBZIoLFdyLS4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMrMjXeujDY3"
      },
      "source": [
        "<br>\n",
        "\n",
        "# **3. PRE-PROCESSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ_FxLrRvsJy"
      },
      "source": [
        "It is not possible to send a Word document to Claude. These steps below convert the document into \"little chunks\" of markdown, ready to send to Claude for correction one chunk at a time.\n",
        "\n",
        "<!-- /Figure with caption 300px width-->\n",
        "<figure>\n",
        "  <figcaption>Fig 4: Pre-processing up to \"little chunks\"</figcaption>\n",
        "  <a href=\"https://michellepace.github.io/word-document-corrector-claude/images/initial-solution-sketch.jpg\"\n",
        "     target=\"_blank\">\n",
        "    <img src=\"https://michellepace.github.io/word-document-corrector-claude/images/initial-solution-sketch.jpg\"\n",
        "         width=\"500\"\n",
        "         alt=\"Initial Solution Sketch\" />\n",
        "  </a>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHzZF0d8_Nyq"
      },
      "source": [
        "## Extract Word doc text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Preserve the same headings, bullet lists, and paragraphs as in the Word document.\n",
        "- For simplicity, ignore everything else like text found in tables, images, charts, headers and footers."
      ],
      "metadata": {
        "id": "fC-xIwyrwWJ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YFKT4cmQ_2F6",
        "outputId": "1036d34d-592f-4019-c834-fd727f29bc6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! â€¢ Extracted text from: Word.docx\n",
            "\n",
            "EXRACTION SUMMARY\n",
            " Total styles: 4\n",
            " Total paragraphs: 117\n",
            " Total word count: 3,506\n",
            "\n",
            "Style            Heading Level     Paragraphs    Words\n",
            "--------------  ---------------  ------------  -------\n",
            "Heading 1              1                    7       35\n",
            "Heading 2              2                   16       66\n",
            "List Paragraph                             21      242\n",
            "Normal                                     73    3,163\n"
          ]
        }
      ],
      "source": [
        "def extract_docx_paragraphs(docx_file: Path) -> list[dict]:\n",
        "    \"\"\"Extract non-empty paragraphs from Word document\"\"\"\n",
        "    doc = docx.Document(docx_file)\n",
        "\n",
        "    def get_heading_level(para):\n",
        "        if para.style.name.startswith('Heading'):\n",
        "            try:\n",
        "                return int(para.style.name.split()[-1])\n",
        "            except ValueError:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    return [{\n",
        "        'text': para.text,\n",
        "        'style': para.style.name,\n",
        "        'heading_level': get_heading_level(para),\n",
        "        'word_count': len(para.text.split())\n",
        "    } for para in doc.paragraphs if para.text.strip()]\n",
        "\n",
        "def print_summary(paragraphs: list[dict], docx_file: Path) -> None:\n",
        "    \"\"\"Print document summary with style statistics\"\"\"\n",
        "    # Calculate stats and build table\n",
        "    style_stats = {}\n",
        "    for style in set(p['style'] for p in paragraphs):\n",
        "        style_paras = [p for p in paragraphs if p['style'] == style]\n",
        "        style_stats[style] = {\n",
        "            'level': style_paras[0]['heading_level'],\n",
        "            'para_count': len(style_paras),\n",
        "            'words': sum(p['word_count'] for p in style_paras)\n",
        "        }\n",
        "\n",
        "    rows = [[\n",
        "        style,\n",
        "        stats['level'] or '',\n",
        "        f\"{stats['para_count']:,}\",\n",
        "        f\"{stats['words']:,}\"\n",
        "    ] for style, stats in sorted(style_stats.items())]\n",
        "\n",
        "    print(f\"Success! â€¢ Extracted text from: {docx_file.name}\\n\")\n",
        "    print(f\"EXRACTION SUMMARY\")\n",
        "    print(f\" Total styles: {len(style_stats)}\")\n",
        "    print(f\" Total paragraphs: {len(paragraphs):,}\")\n",
        "    print(f\" Total word count: {sum(stats['words'] for stats in style_stats.values()):,}\\n\")\n",
        "\n",
        "\n",
        "    print(tabulate(\n",
        "        rows,\n",
        "        headers=['Style', 'Heading Level', 'Paragraphs', 'Words'],\n",
        "        tablefmt='simple',\n",
        "        colalign=('left', 'center', 'right', 'right')\n",
        "    ))\n",
        "\n",
        "# Use the functions\n",
        "docx_paragraphs = extract_docx_paragraphs(my_input_docx_file)\n",
        "print_summary(docx_paragraphs, my_input_docx_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMlv6PpbR1KR"
      },
      "source": [
        "## Convert to Markdown\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the extracted test into a [markdown]( https://markdownguide.offshoot.io/getting-started) file.\n",
        "\n",
        "Markdown format was chosen because:\n",
        "1. Can preserve Word document structure (headings, lists, paragraphs)\n",
        "1. It's a clean format that large language models can read efficiently (unlike HTML).\n",
        "1. You can mostly avoid splitting a heading section into multiple chunks, Claude will have more awareness of context for corrections."
      ],
      "metadata": {
        "id": "YGTttsz8wjb4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wxnllZMwSOoL",
        "outputId": "2a6ed3ea-5e15-47b7-8718-fe5f03bfdc60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "â€¢ Word document text saved as a markdown file\n",
            "â€¢ It is here: \"/content/drive/MyDrive/TEST/Word.docx.ORIG.md\"\n"
          ]
        }
      ],
      "source": [
        "def create_simple_markdown_file(paragraphs: list[dict], markdown_file: Path) -> Path:\n",
        "    \"\"\" Convert paragraphs to Markdown format and save to a file.\n",
        "    :param paragraphs: List of dictionaries containing paragraph information\n",
        "    :param markdown_file: Path to save the generated Markdown file\n",
        "    :return: Path of the created Markdown file\n",
        "    \"\"\"\n",
        "    def format_paragraph(para):\n",
        "        if para['heading_level'] is not None:\n",
        "            return f\"{'#' * para['heading_level']} {para['text']}\"\n",
        "        elif para['style'].startswith('List'):\n",
        "            return f\"- {para['text']}\"\n",
        "        else:\n",
        "            return para['text']\n",
        "\n",
        "    # Filter out empty paragraphs and format the rest\n",
        "    formatted_paragraphs = [format_paragraph(para) for para in paragraphs if para['text'].strip()]\n",
        "\n",
        "    markdown_content = []\n",
        "    for i, current_para in enumerate(formatted_paragraphs):\n",
        "        if i > 0:\n",
        "            prev_is_list = formatted_paragraphs[i-1].startswith(\"- \")\n",
        "            current_is_list = current_para.startswith(\"- \")\n",
        "            # Single newline for consecutive list items, double for others\n",
        "            markdown_content.append(\"\\n\" if prev_is_list and current_is_list else \"\\n\\n\")\n",
        "\n",
        "        markdown_content.append(current_para)\n",
        "\n",
        "    # Join paragraphs and normalise spacing:\n",
        "    markdown_content = \"\".join(markdown_content)\n",
        "    # Remove consecutive empty lines and strip leading/trailing whitespace\n",
        "    markdown_content = re.sub(r'\\n{3,}', '\\n\\n', markdown_content.strip())\n",
        "\n",
        "    markdown_file.write_text(markdown_content, encoding='utf-8')\n",
        "    return markdown_file\n",
        "\n",
        "\n",
        "### Do the work\n",
        "original_md_file = create_simple_markdown_file(docx_paragraphs, original_md_file)\n",
        "print(f'Success!\\nâ€¢ Word document text saved as a markdown file\\nâ€¢ It is here: \"{original_md_file.absolute()}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split into Chunks"
      ],
      "metadata": {
        "id": "yMTww-Cxw9Cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Claude 3.5 Sonnet allows up to 8,192 tokens (~6,000 words), using smaller chunks makes it easier to compare original and processed text.\n",
        "\n",
        "**Character-Based Chunking**\n",
        "\n",
        "I use character-based chunking instead of tokens to avoid breaking markdown elements (headings, lists, etc.). The markdown-splitting library used only supports character-based splits.\n",
        "\n",
        "**Target Chunk Size**\n",
        "\n",
        "A starting guess of 600 words (just over one page) was used. Then based on token analysis using the Anthropic Tokenizer (Figure 5) I determined:\n",
        "- Target: 600 words (~4,400 characters) per chunk\n",
        "- Total prompt size: Prompt template (269 words) + chunk (600 words) â‰ˆ 1,100 tokens\n",
        "- Well below Claude's 8,192 token limit\n",
        "- Prompt instructs to \"preserve meaning\": response size will be < 8,192 tokens too.\n",
        "- Set target chunk size: 4,400 characters.\n",
        "\n",
        "<!-- /Figure with caption full width-->\n",
        "<figure>\n",
        "  <figcaption>Fig 5: Table For Estimating Conversion: Between words, characters, and Anthropic Tokens</figcaption>\n",
        "    <img src=\"https://michellepace.github.io/word-document-corrector-claude/images/text-analysis-with-tokeniser.jpg\"\n",
        "         alt=\"Estimating conversion rates between words, characters, and Anthropic Tokens\" />\n",
        "  </a>\n",
        "</figure>\n",
        "\n",
        "**Note:** While smaller chunks increase API costs due to more \"prompt template\" submissions, they improve quality control by making semantic changes easier to spot."
      ],
      "metadata": {
        "id": "2vsAOcvZw2Ji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l7GP2q8luuBF",
        "outputId": "13d0d2e9-b3c6-4aa7-a70f-46432472be32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "â€¢ The markdown file containg your Word document text has been split into 5 manageable chunks.\n",
            "â€¢ Each chunk aims to be around 4,400 characters long (never more).\n",
            "â€¢ These chunks are now ready to be sent to Claude for processing (ie correction).\n",
            "\n",
            "\n",
            "Summary of All Chunks\n",
            "------------------------------------------------\n",
            "What               Characters    Tokens    Count\n",
            "---------------  ------------  --------  -------\n",
            "Total Chunks           20,747     4,755        5\n",
            "Avg. Chunk size         4,149       951        *\n",
            "Min. Chunk size         3,584       758        *\n",
            "Max. Chunk size         4,395     1,069        *\n",
            "\n",
            "\n",
            "Summary per Chunk\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "Chunk            Lines       Words       Chars      Tokens  Chunk Start\n",
            "----------  ----------  ----------  ----------  ----------  --------------------------------------------\n",
            "Chunk 1             48         689       4,177         956  # ğŸŸ¡ A Story with Claude & Usage  ## What...\n",
            "Chunk 2             51         754       4,395       1,005  If you are new to AI development, don't ...\n",
            "Chunk 3             48         749       4,288       1,069  In the output of the code below you can ...\n",
            "Chunk 4             43         752       4,303         967  To get around my nerves, I told Claude t...\n",
            "Chunk 5             23         606       3,584         758  Moreover, even if I stick with Claude, h...\n",
            "----------  ----------  ----------  ----------  ----------  ----------\n",
            "Total              213       3,550      20,747       4,755\n"
          ]
        }
      ],
      "source": [
        "def split_markdown_into_chunks(\n",
        "    markdown_file: Path,\n",
        "    target_chunk_size_chars: int = 2000,\n",
        "    chunk_overlap: int = 0\n",
        ") -> list[str]:\n",
        "    \"\"\" Split a Markdown file into text chunks using MarkdownTextSplitter.\n",
        "    :param markdown_file: Path to the Markdown file\n",
        "    :param target_chunk_size_chars: Target size of each chunk in characters\n",
        "    :param chunk_overlap: Number of overlapping characters between chunks\n",
        "    :return: List of text chunks (that don't exceed the target chunk size)\n",
        "    \"\"\"\n",
        "    text = markdown_file.read_text(encoding='utf-8')\n",
        "    text_splitter = MarkdownTextSplitter(chunk_size=target_chunk_size_chars, chunk_overlap=chunk_overlap)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    print(f\"Success!\")\n",
        "    print(f\"â€¢ The markdown file containg your Word document text has been split into {len(chunks)} manageable chunks.\")\n",
        "    print(f\"â€¢ Each chunk aims to be around {target_chunk_size_chars:,} characters long (never more).\")\n",
        "    print(f\"â€¢ These chunks are now ready to be sent to Claude for processing (ie correction).\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def count_claude_tokens(text: str) -> int:\n",
        "    count = MY_ANTHROPIC_CLIENT.beta.messages.count_tokens(\n",
        "        model=CLAUDE_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": text}]\n",
        "    )\n",
        "    return count.input_tokens\n",
        "\n",
        "\n",
        "def print_chunking_summary(chunks: list[str]) -> None:\n",
        "    sizes = [len(chunk) for chunk in chunks]\n",
        "    tokens = [count_claude_tokens(chunk) for chunk in chunks]\n",
        "\n",
        "    print(f\"\\n\\nSummary of All Chunks\\n{'-' * 48}\")\n",
        "    print(tabulate([\n",
        "        [\"Total Chunks\", f\"{sum(sizes):,}\", f\"{sum(tokens):,}\", f\"{len(chunks)}\"],\n",
        "        [\"Avg. Chunk size\", f\"{sum(sizes)/len(chunks):,.0f}\", f\"{sum(tokens)/len(chunks):,.0f}\", \"*\"],\n",
        "        [\"Min. Chunk size\", f\"{min(sizes):,}\", f\"{min(tokens):,}\", \"*\"],\n",
        "        [\"Max. Chunk size\", f\"{max(sizes):,}\", f\"{max(tokens):,}\", \"*\"]\n",
        "    ],\n",
        "        headers=['What', 'Characters', 'Tokens', 'Count'],\n",
        "        tablefmt='simple',\n",
        "        colalign=('left', 'right', 'right', 'right')\n",
        "    ))\n",
        "\n",
        "\n",
        "def print_chunk_table(chunks: list[str]) -> None:\n",
        "    # Generate chunk statistics rows\n",
        "    rows = [[\n",
        "        f\"Chunk {i}\",\n",
        "        f\"{len(chunk.splitlines()):,}\",\n",
        "        f\"{len(chunk.split()):,}\",\n",
        "        f\"{len(chunk):,}\",\n",
        "        f\"{count_claude_tokens(chunk):,}\",\n",
        "        chunk.replace('\\n', ' ').replace('\\r', '')[:40] + \"...\"\n",
        "    ] for i, chunk in enumerate(chunks, 1)]\n",
        "\n",
        "    # Add separator and totals row\n",
        "    rows.append([\"-\" * 10] * 6)  # Add separator row\n",
        "    rows.append([\n",
        "        \"Total\"\n",
        "    ] + [f\"{sum(int(row[i].replace(',','')) for row in rows[:-1]):,}\" for i in range(1, 5)] + [\"\"])\n",
        "\n",
        "    print(f\"\\n\\nSummary per Chunk\\n{'-' * 104}\")\n",
        "    print(tabulate(\n",
        "        rows,\n",
        "        headers=['Chunk', 'Lines', 'Words', 'Chars', 'Tokens', 'Chunk Start'],\n",
        "        tablefmt='simple',\n",
        "        colalign=('left', 'right', 'right', 'right', 'right', 'left')\n",
        "    ))\n",
        "\n",
        "### Do the work\n",
        "original_chunks = split_markdown_into_chunks(original_md_file, target_chunk_size_chars=TARGET_CHUNK_CHARACTERS)\n",
        "print_chunking_summary(original_chunks)\n",
        "print_chunk_table(original_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnT_BhdtcqVb"
      },
      "source": [
        "<br>\n",
        "\n",
        "- Uncomment the below to see inside a specified chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_M4DJY_0c4um"
      },
      "outputs": [],
      "source": [
        "def print_chunks(chunks: list[str], chunks_to_print: list[int] | None = None) -> None:\n",
        "    total_chunks = len(chunks)\n",
        "\n",
        "    if chunks_to_print is None:\n",
        "        chunks_to_print = list(range(1, total_chunks + 1))\n",
        "\n",
        "    print(\"\\nCHUNK CONTENTS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for chunk_number, chunk in enumerate(chunks, start=1):\n",
        "        if chunk_number in chunks_to_print:\n",
        "            print(f\"\\nğŸŒ¸ CHUNK {chunk_number} contains:\")\n",
        "            print(\"-\" * 80)\n",
        "            print(f\"\\n{chunk}\\n\")\n",
        "\n",
        "    # Error messages for non-existent chunks\n",
        "    invalid_chunks = [n for n in chunks_to_print if n > total_chunks]\n",
        "    if invalid_chunks:\n",
        "        print(\"\\nERROR:\")\n",
        "        print(f\"Requested chunks {invalid_chunks} do not exist (max: {total_chunks})\")\n",
        "\n",
        "\n",
        "# Example usage: show chunks 1, 3\n",
        "# print_chunks(original_chunks, [1, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqQcPe9Qu78I"
      },
      "source": [
        "<br>\n",
        "\n",
        "# **4. PROCESSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT0_0CjEvedY"
      },
      "source": [
        "## Prompt Template (+chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual chunks are embedded into the prompt template. That means, if there are say 20 chunks, then Claude will be prompted 20 times. The output below shows an example of what the total prompt sent to Claude looks like."
      ],
      "metadata": {
        "id": "0cAFMxu7xo4Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3fMTLYsxv22X",
        "outputId": "b4ef3e28-5ff3-44a9-c609-b6a370f9e942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Between solid lines: Prompt sent to Claude (Prompt template + example chunk)\n",
            "__________________________________________________________________________________________ \n",
            "\n",
            "CRITICAL: PROVIDE ONLY THE CORRECTED TEXT WITHOUT ANY ADDITIONAL COMMENTARY.\n",
            "\n",
            "Your task is to take the provided text and rewrite it into a clear, grammatically correct version while preserving the original meaning as closely as possible. Correct any spelling mistakes, punctuation errors, verb tense issues, word choice problems, and other grammatical mistakes.\n",
            "\n",
            "MANDATORY INSTRUCTIONS:\n",
            "\n",
            "1. Determine and use the same linguistic language as the original text (e.g., English, German)\n",
            "2. Preserve all existing markdown formatting, including heading levels, paragraphs, and lists\n",
            "3. Make necessary grammatical corrections, including spelling, punctuation, verb tense, word choice, and other grammatical issues. Only make stylistic changes if essential for clarity\n",
            "4. Mark corrections with markdown syntax, apply one of these choices only:\n",
            "   - For changed text use bold: e.g., **changed** and **multiple changed words**\n",
            "   - For new text use bold: **new words**\n",
            "   - For removed text use bold strikethrough: **~~removed words~~**\n",
            "5. Maintain the original structure:\n",
            "   - Don't add new lines of text\n",
            "   - Don't include additional commentary at all\n",
            "   - Don't convert markdown elements to different types\n",
            "6. For ambiguous corrections, choose the option that best preserves original meaning and style\n",
            "7. Ensure consistency in corrections throughout the text\n",
            "8. Return the corrected text in markdown syntax\n",
            "9. DO NOT add any explanations, introductions, or conclusions to your response\n",
            "\n",
            "FINAL REMINDER: Your output should consist SOLELY of the corrected text. Do not include phrases like \"Here is the corrected text\" or any other form of commentary.\n",
            "\n",
            "The text to be corrected is provided between the triple tildes (~~~):\n",
            "\n",
            "~~~\n",
            "# I'M A LITTLE EXAMPLE CHUNK OF MARKDOWN TEXT TELLING A STORY.\n",
            "- Chunks are placed in between the '~~~' band, this is exactly where I am right now.\n",
            "- Text above or below the '~~~' band is the same for all prompts sent to Claude.\n",
            "- To make a chunk, the Word document was converted into markdown and split into pieces (ie chunks)\n",
            "- Target chunk size was set to a maximum of 4400 characters, respecting markdown elements.\n",
            "- If there are 19 chunks, Claude will be prompted 19 seperate times and return 19 corrected chunks.\n",
            "- Corrected chunks are reassembled together and then converted into html with corrections in colour.\n",
            "- Again, I'm just an **example** chunk. The real ones will be sent to Claude. Not me!\n",
            "~~~\n",
            "\n",
            "REMEMBER: Provide ONLY the corrected text without any additional words or explanations.\n",
            "__________________________________________________________________________________________ \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def print_prompt_with_chunk_example():\n",
        "    example_chunk = f\"\"\"# I'M A LITTLE EXAMPLE CHUNK OF MARKDOWN TEXT TELLING A STORY.\n",
        "- Chunks are placed in between the '~~~' band, this is exactly where I am right now.\n",
        "- Text above or below the '~~~' band is the same for all prompts sent to Claude.\n",
        "- To make a chunk, the Word document was converted into markdown and split into pieces (ie chunks)\n",
        "- Target chunk size was set to a maximum of {TARGET_CHUNK_CHARACTERS} characters, respecting markdown elements.\n",
        "- If there are 19 chunks, Claude will be prompted 19 seperate times and return 19 corrected chunks.\n",
        "- Corrected chunks are reassembled together and then converted into html with corrections in colour.\n",
        "- Again, I'm just an **example** chunk. The real ones will be sent to Claude. Not me!\"\"\"\n",
        "\n",
        "    prompt_with_chunk = PROMPT_TEMPLATE.format(the_markdown_chunk=example_chunk)\n",
        "    print(prompt_with_chunk)\n",
        "\n",
        "\n",
        "### Do the work\n",
        "print(\"Between solid lines: Prompt sent to Claude (Prompt template + example chunk)\")\n",
        "print(\"_\" * 90, \"\\n\")\n",
        "print_prompt_with_chunk_example()\n",
        "print(\"_\" * 90, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process Chunks"
      ],
      "metadata": {
        "id": "mJ3IviyrxuX4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z68oCyMxNnC"
      },
      "source": [
        "Each chunk is embedded in the base prompt and sent to Claude with instructions to make corrections. The processed chunks are collected in order and later reassembled into a complete document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MRlggTtTyKw1",
        "outputId": "812be57d-b8ce-459f-8e74-067d0b260ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f45c26c9dc3e4b9fa6f9ab8faa02e446",
            "04cbed82b858440ab636b502c7546231",
            "a7ad99bd6f4545a18c1b6715f2685b79",
            "381fc148925a4b7287fcc2b784eaa454",
            "9576bd96a21c42f8836c638f928bb205",
            "a006df887dca4029a63097b61b8ae92e",
            "d9e16fbcd44646978b1ed70e6e280ce6",
            "44988c147a2c461c890f3819cdf34bc2",
            "1acd3f8c9fa044ce824663ee01fc3485",
            "3bfa325b4a234c3bbe84d008b403b4a8",
            "2dff3f98bc60431da7d358c8ea2ff08e"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing: sending text chunks to Claude for correction! :   0%|           0/5 Chunks"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f45c26c9dc3e4b9fa6f9ab8faa02e446"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete: 5/5 chunks processed successfully.\n"
          ]
        }
      ],
      "source": [
        "def process_all_chunks(chunks: list[str]) -> list[str]:\n",
        "    \"\"\" Process all chunks of markdown text and show progress bar.\n",
        "    :param chunks: List of markdown text chunks\n",
        "    :return: List of processed text chunks (error message for failed chunks)\n",
        "    \"\"\"\n",
        "    processed_chunks = []\n",
        "    processing_desc = \"Processing: sending text chunks to Claude for correction! \"\n",
        "    for i, chunk in tqdm(enumerate(chunks, 1), total=len(chunks), desc=processing_desc, bar_format='{l_bar}{bar} {n_fmt}/{total_fmt} Chunks'):\n",
        "        result = process_one_chunk(i, chunk)\n",
        "        processed_chunks.append(result)\n",
        "\n",
        "    successful_chunks = sum(1 for chunk in processed_chunks if not chunk.startswith(\"ERROR\"))\n",
        "    total_chunks = len(chunks)\n",
        "\n",
        "    if successful_chunks == total_chunks:\n",
        "        print(f\"Processing complete: {successful_chunks}/{total_chunks} chunks processed successfully.\")\n",
        "    else:\n",
        "        print(f\"Processing complete: Warning!! Only {successful_chunks}/{total_chunks} chunks were processed successfully.\")\n",
        "\n",
        "    return processed_chunks\n",
        "\n",
        "def process_one_chunk(\n",
        "    chunk_count: int,\n",
        "    chunk: str,\n",
        "    client: anthropic.Anthropic = MY_ANTHROPIC_CLIENT,\n",
        "    model: str = CLAUDE_MODEL,\n",
        "    temperature: float = PROMPT_TEMP,\n",
        "    max_tokens: int = 4096 # This is Claude Sonnet's maxiumum size of text measured in tokens he is able to reply with.\n",
        ") -> str:\n",
        "    \"\"\" Process a single chunk of markdown text using the Anthropic API.\n",
        "    :param chunk_count: The index of the current chunk\n",
        "    :param chunk: The markdown text to process\n",
        "    :param client: The Anthropic client (default: MY_ANTHROPIC_CLIENT)\n",
        "    :param model: The Claude model to use (default: CLAUDE_MODEL)\n",
        "    :param temperature: The temperature setting for the model (default: PROMPT_TEMP)\n",
        "    :param max_tokens: The maximum number of tokens in the response (default: PROMPT_MAX_RESPONSE_TOKENS)\n",
        "    :return: Processed text or error message if processing failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.messages.create(\n",
        "            model=model,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": PROMPT_TEMPLATE.format(the_markdown_chunk=chunk)}]\n",
        "            }]\n",
        "        )\n",
        "        if not (response.content and response.content[0].text):\n",
        "            raise ValueError(f\"Empty response from API for chunk {chunk_count}\")\n",
        "\n",
        "        return response.content[0].text\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = get_friendly_error_msg(e, chunk_count)\n",
        "        print(error_message)  # Print error message for logging\n",
        "        return error_message  # Return error message for failed chunks\n",
        "\n",
        "def get_friendly_error_msg(exception: Exception, chunk_count: int) -> str:\n",
        "    CHUNK_ERROR_TEMPLATE = \"ERROR (chunk {}) was not processed because {}\"\n",
        "\n",
        "    error_msgs = {\n",
        "        # Found in docs Jan 2025\n",
        "        anthropic.AuthenticationError: \"there's an issue with your Anthropic API key\",\n",
        "        anthropic.PermissionDeniedError: \"your API key does not have permission to use the specified resource\",\n",
        "        anthropic.RateLimitError: \"your account has hit a Rate limit. It's measured in requests/minute, tokens/minute, and tokens/day\",\n",
        "        anthropic.InvalidRequestError: \"there was an issue with the format or content of your request\",\n",
        "        anthropic.NotFoundError: \"the requested resource was not found\",\n",
        "        anthropic.RequestTooLargeError: \"request exceeds the maximum allowed number of bytes\",\n",
        "        anthropic.APIError: \"an unexpected error has occurred internal to Anthropic's systems\",\n",
        "        anthropic.OverloadedError: \"Anthropic's API is temporarily overloaded. Please retry your request after a short delay\",\n",
        "        # Can no longer find these in the docs, but keeping for in case.\n",
        "        anthropic.APITimeoutError: \"Anthropic took too long to respond. Likely an issue on Anthropic's end.\",\n",
        "        anthropic.APIConnectionError: \"failed to connect to Anthropic's API. Perhaps network issues on your end\",\n",
        "        anthropic.APIStatusError: \"Anthropic returned an unsuccessful status code\",\n",
        "    }\n",
        "\n",
        "    for error_type, msg in error_msgs.items():\n",
        "        if isinstance(exception, error_type):\n",
        "            error_msg = f\"{msg}: {exception}\"\n",
        "            break\n",
        "    else:\n",
        "        if isinstance(exception, ValueError) and \"Empty response\" in str(exception):\n",
        "            error_msg = f\"Anthropic returned an empty response for chunk {chunk_count} (maybe we sent an empty chunk?): {exception}\"\n",
        "        else:\n",
        "            error_msg = f\"Unexpected error on chunk {chunk_count}: {exception}\"\n",
        "\n",
        "    return CHUNK_ERROR_TEMPLATE.format(chunk_count, error_msg)\n",
        "\n",
        "\n",
        "### Do the work\n",
        "processed_chunks = process_all_chunks(chunks=original_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1eHTvPYyabB"
      },
      "source": [
        "<br>\n",
        "\n",
        "# **5. POST-PROCESSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFbbAbzLydvC"
      },
      "source": [
        "## Reassemble Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reassemble processed (corrected) chunks into markdown file."
      ],
      "metadata": {
        "id": "7aSaikXTyK91"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4tA2N5jQyroO",
        "outputId": "08f69c86-20dc-4e2f-f3b0-23abec15b0d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "â€¢ Processed chunks assembled into one file:\n",
            "â€¢ here it is: \"/content/drive/MyDrive/TEST/Word.docx.PROCESSED.md\"\n"
          ]
        }
      ],
      "source": [
        "def reassemble_chunks(chunks: list[str], output_file: Path) -> Path:\n",
        "    \"\"\" Reassemble processed chunks of text and save to an output file.\n",
        "    :param chunks: List of processed markdown text chunks\n",
        "    :param output_file: Path to the output file where the reassembled text will be saved\n",
        "    :return: Path to the output file containing the reassembled text\n",
        "    \"\"\"\n",
        "    # Join chunks with double newlines\n",
        "    reassembled_chunks = \"\\n\\n\".join(chunks)\n",
        "\n",
        "    # Remove consecutive empty lines\n",
        "    reassembled_chunks = re.sub(r'\\n{3,}', '\\n\\n', reassembled_chunks)\n",
        "\n",
        "    # Remove leading and trailing whitespace\n",
        "    reassembled_chunks = reassembled_chunks.strip()\n",
        "\n",
        "    # Save the corrected document\n",
        "    output_file.write_text(reassembled_chunks, encoding='utf-8')\n",
        "\n",
        "    return output_file\n",
        "\n",
        "\n",
        "### Do the work\n",
        "processed_md_file = reassemble_chunks(processed_chunks, processed_md_file)\n",
        "print(f\"Success!\")\n",
        "print(f\"â€¢ Processed chunks assembled into one file:\")\n",
        "print(f'â€¢ here it is: \"{processed_md_file.absolute()}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corrections In Colour"
      ],
      "metadata": {
        "id": "lIQjmq8XyQyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert markdown file into html so corrections are in colour and easy to spot."
      ],
      "metadata": {
        "id": "sUN6_5whyUon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_markdown_to_html(markdown_file: Path, html_file: Path) -> Path:\n",
        "    \"\"\" Convert a Markdown file to HTML with custom styling and preprocessing.\n",
        "    :param markdown_file: Path object representing the input Markdown file\n",
        "    :param html_file: Path object representing the output HTML file\n",
        "    :return: Path object of the output HTML file\n",
        "    \"\"\"\n",
        "    # Read the Markdown file\n",
        "    markdown_text = markdown_file.read_text(encoding='utf-8')\n",
        "\n",
        "    # Convert custom Markdown strikethrough syntax to HTML, handling regular and bold strikethroughs\n",
        "    def process_strikethrough(match):\n",
        "        content = match.group(2)\n",
        "        is_bold = bool(match.group(1))\n",
        "        if is_bold:\n",
        "            return f\"<strong><del>{content}</del></strong>\"\n",
        "        else:\n",
        "            return f\"<del>{content}</del>\"\n",
        "\n",
        "    strikethrough_pattern = r'(\\*\\*)?~~(.*?)~~(\\*\\*)?'\n",
        "    markdown_text = re.sub(strikethrough_pattern, process_strikethrough, markdown_text)\n",
        "\n",
        "    # Convert to HTML\n",
        "    html_body = markdown(markdown_text)\n",
        "\n",
        "    # Define CSS styles, including the improved responsive design\n",
        "    css_styles = \"\"\"\n",
        "    <style>\n",
        "        body {\n",
        "            margin: 0 auto;\n",
        "            padding: 0 5%;\n",
        "            max-width: 50em;\n",
        "            line-height: 1.5em;\n",
        "            font-family: 'Inter', Arial, sans-serif;\n",
        "            font-size: 16px;\n",
        "            background-color: #F0EFEA;\n",
        "            color: #141413;\n",
        "        }\n",
        "\n",
        "        @media (max-width: 768px) { body { padding: 0 3%;}}\n",
        "\n",
        "        strong { color: #E46264;}\n",
        "    </style>\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the full HTML document\n",
        "    full_html = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "        <title>Corrected Document</title>\n",
        "        <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
        "        {css_styles}\n",
        "    </head>\n",
        "    <body>\n",
        "        {html_body}\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # Write the HTML to a file\n",
        "    html_file.write_text(full_html, encoding='utf-8')\n",
        "\n",
        "    return html_file\n",
        "\n",
        "def download_drive_file(file_path: Path):\n",
        "    file_path = Path(file_path)\n",
        "\n",
        "    if not file_path.is_file():\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return\n",
        "\n",
        "    # Download file directly from Google Drive\n",
        "    files.download(str(file_path))\n",
        "\n",
        "    print(f\"â€¢ Browser download initiated...\")\n",
        "\n",
        "\n",
        "### Do the work\n",
        "processed_html_file = convert_markdown_to_html(processed_md_file, processed_html_file)\n",
        "print(f\"Success!\")\n",
        "print(f\"â€¢ Saved corrected Word document as: {processed_html_file.absolute()}\")\n",
        "\n",
        "download_drive_file(processed_html_file)\n",
        "print(f\"â€¢ Downloaded as: {processed_md_file.name}\")\n",
        "print(f\"â€¢ Enjoy the corrections Word missed, we're done! ğŸ™‚\")"
      ],
      "metadata": {
        "id": "3VBWUxCvyYd5",
        "outputId": "eac3a080-179d-4a5e-cd65-c3448dd283c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "â€¢ Saved corrected Word document as: /content/drive/MyDrive/TEST/Word.docx.PROCESSED.html\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_12ee9ff9-1d29-4391-b961-5897440ed4db\", \"Word.docx.PROCESSED.html\", 23778)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â€¢ Browser download initiated...\n",
            "â€¢ Downloaded as: Word.docx.PROCESSED.md\n",
            "â€¢ Enjoy the corrections Word missed, we're done! ğŸ™‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03brUgd-scNQ"
      },
      "source": [
        "<br>\n",
        "\n",
        "# **6. TESTING**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"id-test-plan\"></a>\n",
        "## Test Plan (criticalâ™¥ï¸)"
      ],
      "metadata": {
        "id": "Rz0iHyW4yjZD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lpVevOG7wSA"
      },
      "source": [
        "What I have learnt through creating this notebook is that â€” automated testing â€” especially when working with LLMs â€” is the difference between success and failure. You must go slower to go faster.\n",
        "<br>\n",
        "\n",
        "The test plan is divided into 3 major areas:\n",
        "\n",
        "1. **Test Processed Doc:** End-to-end testing, verifying code and prompt.\n",
        "1. **Prompt testing:** Prompt evaluation to ensure responses are as expected (generated test data).\n",
        "1. **Test My Code:** Traditional functional testing for core code components.\n",
        "\n",
        "<br>\n",
        "\n",
        "The first two major areas incorporate testing the response from Claude. It may seem like overkill but I quite disagree, and here is why:\n",
        "\n",
        "- What if I have 20 large Word documents to correct? It is not practical to manually check each one. What if my document is in Italian? Should I assume Claude will perform as well, or should I have an automated way of testing it? Just because I've thoroughly tested one document, that doesn't imply the same level of dependability for another as content varies vastly.\n",
        "\n",
        "- As a Product Manager, what if I decide to switch to the free Meta Llama 3.1 model instead of Claude Sonnet, or a one tenth of the cost model like DeepSeek v3? How would I have any chance of knowing that my \"product\" can perform as well as it did with Claude 3.5 Sonnet? The same can be said for model updates.\n",
        "\n",
        "- Manually testing for all these scenarios is time-consuming. Automated tests increased the quality of corrections across a wide range of documents and made me more confident to adopt model updates (and a model switch soon).\n",
        "\n",
        "- Testing is marked with a heart \"â™¥ï¸\" because: It must be at the heart of a product when leveraging an LLM â€” the have non-deterministic responses.\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, the test plan was developed in collaboration with Claude:\n",
        "<!-- /Figure with caption 200px width-->\n",
        "<figure>\n",
        "  <figcaption>Fig 6: Test Plan Brainstorming</figcaption>\n",
        "  <a href=\"https://michellepace.github.io/word-document-corrector-claude/images/test-plan-brainstorming.jpg\"\n",
        "     target=\"_blank\">\n",
        "    <img src=\"https://michellepace.github.io/word-document-corrector-claude/images/test-plan-brainstorming.jpg\"\n",
        "         width=\"200\"\n",
        "         alt=\"Talking testing with Claude and designing a test plan.\" />\n",
        "  </a>\n",
        "</figure>\n",
        "\n",
        "<!-- /Figure with caption 200px width-->\n",
        "<figure>\n",
        "  <figcaption>Fig 7: Final Test Plan</figcaption>\n",
        "  <a href=\"https://michellepace.github.io/word-document-corrector-claude/images/final-test-plan.jpg\"\n",
        "     target=\"_blank\">\n",
        "    <img src=\"https://michellepace.github.io/word-document-corrector-claude/images/final-test-plan.jpg\"\n",
        "         width=\"200\"\n",
        "         alt=\"Sketched out test plan using what I learnt from Claude.\" />\n",
        "  </a>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"id-test-processed-doc\"></a>\n",
        "## 6.1 Test Processed Doc"
      ],
      "metadata": {
        "id": "Wzzxv8P-ytoV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KpO_lr30Cn-"
      },
      "source": [
        "Test that Claude has enhanced and corrected writing without changing the core structure or meaning, specifically:\n",
        "\n",
        "1. **a) Structure Preservation:** Are the headings and paragraphs still in the right place?\n",
        "1. **b) Content Preservation: Word count:** Has the overall length changed dramatically?\n",
        "1. **c) Content Preservation: Semantic meaning:** Does the corrected text still mean the same?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Content Preservation: Document Structure"
      ],
      "metadata": {
        "id": "3I1iR_BHy9er"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw0qXt2ZHQ5V"
      },
      "source": [
        "Checks the structure of the Word document has remained intact, like headings and bullet lists. Some structural changes may be valid. For example, if you accidentally started a new paragraph mid-sentence in your Word document, Claude would likely fix that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XAWH69OyGz97",
        "outputId": "f9d68f09-f0d8-4f2f-baba-c35e39e69ab0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluate Content Preservation: By Document Structure\n",
            "------------------------------------------------------------------\n",
            "Status: MATCH\n",
            "\n",
            "Markdown Element      Original    Processed   Match\n",
            "------------------  ----------  -----------  -------\n",
            "Headings                    23           23     âœ“\n",
            "Paragraphs                  73           73     âœ“\n",
            "List Items                  21           21     âœ“\n",
            "Total Lines                217          217     âœ“\n",
            "Empty Lines                100          100     âœ“\n"
          ]
        }
      ],
      "source": [
        "def display_markdown_structure_comparison(original_file: Path, processed_file: Path) -> None:\n",
        "    \"\"\"Test if document structure is preserved after processing.\"\"\"\n",
        "\n",
        "    def count_markdown_elements(text: str) -> dict[str, int]:\n",
        "        lines = text.splitlines()\n",
        "        heading_pattern = re.compile(r'^#+\\s')\n",
        "\n",
        "        return {\n",
        "            'headings': sum(1 for line in lines if heading_pattern.match(line)),\n",
        "            'paragraphs': sum(1 for line in lines if line.strip() and not heading_pattern.match(line)\n",
        "                            and not line.startswith(('- ', '* '))),\n",
        "            'list_items': sum(1 for line in lines if line.startswith(('- ', '* '))),\n",
        "            'total_lines': len(lines),\n",
        "            'empty_lines': sum(1 for line in lines if not line.strip())\n",
        "        }\n",
        "\n",
        "    # Count elements in both files\n",
        "    original = count_markdown_elements(original_file.read_text(encoding='utf-8'))\n",
        "    processed = count_markdown_elements(processed_file.read_text(encoding='utf-8'))\n",
        "\n",
        "    # Prepare comparison table\n",
        "    rows = [[\n",
        "        element.replace('_', ' ').title(),\n",
        "        f\"{original[element]:,}\",\n",
        "        f\"{processed[element]:,}\",\n",
        "        \"âœ“\" if original[element] == processed[element] else \"Ã—\"\n",
        "    ] for element in original]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nEvaluate Content Preservation: By Document Structure\\n{'-' * 66}\")\n",
        "    print(f\"Status: {'MATCH' if original == processed else 'MISMATCH(!)'}\\n\")\n",
        "\n",
        "    print(tabulate(\n",
        "        rows,\n",
        "        headers=['Markdown Element', 'Original', 'Processed', 'Match'],\n",
        "        tablefmt='simple',\n",
        "        colalign=('left', 'right', 'right', 'center')\n",
        "    ))\n",
        "\n",
        "\n",
        "# Do the work\n",
        "display_markdown_structure_comparison(original_md_file, processed_md_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Content Preservation: Simple Word Count"
      ],
      "metadata": {
        "id": "151_YV_XzLER"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFPhBO_iHYdG"
      },
      "source": [
        "Comparison of word count in original document compared to the corrected document. Large differences could mean Claude has not preserved meaning. A simple but effective check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GGH9rX4jHd_b",
        "outputId": "24d52505-cc93-49bf-94c3-2185912e2e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluate Content Preservation: By Simple Word Count\n",
            "------------------------------------------------------------------\n",
            "Chunk      Original    Processed    Difference    Change\n",
            "-------  ----------  -----------  ------------  --------\n",
            "Chunk 1         678          665           -13     -1.9%\n",
            "Chunk 2         742          722           -20     -2.7%\n",
            "Chunk 3         740          740            +0     +0.0%\n",
            "Chunk 4         744          746            +2     +0.3%\n",
            "Chunk 5         602          603            +1     +0.2%\n"
          ]
        }
      ],
      "source": [
        "def word_count_comparison(\n",
        "    original_texts:list[str],\n",
        "    processed_texts: list[str],\n",
        ") -> list[dict[str, str | int | float]]:\n",
        "    \"\"\" Compare word counts between lists of original and corrected texts.\n",
        "    :param original_texts: List of original text contents\n",
        "    :param processed_texts: List of corrected text contents\n",
        "    :return: List of dictionaries containing word count comparison information\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for chunk_number, (original_text, processed_text) in enumerate(zip(original_texts, processed_texts), 1):\n",
        "        original_count = len(original_text.split())\n",
        "        processed_count = len(processed_text.split())\n",
        "        difference = processed_count - original_count\n",
        "        percentage_diff = (difference / original_count * 100) if original_count else 0\n",
        "        sign = '+' if difference >= 0 else ''\n",
        "\n",
        "        results.append({\n",
        "            \"chunk_number\": chunk_number,\n",
        "            \"similarity_test\": \"Word Count\",\n",
        "            \"original_count\": original_count,\n",
        "            \"corrected_count\": processed_count,\n",
        "            \"difference\": difference,\n",
        "            \"percentage_difference\": percentage_diff,\n",
        "            \"message_tabular\": f\"Word Count    Original: {original_count:4d} | Processed: {processed_count:4d} | Difference: {difference:+4d}   {percentage_diff:+.1f}%\",\n",
        "            \"message_arrow\": f\"Word Count: {original_count:4d} â†’ {processed_count:4d}  |  {sign}{difference} words  {sign}{percentage_diff:.0f}%\"\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def strip_off_markdown(chunks: list[str]) -> list[str]:\n",
        "    \"\"\" Remove markdown formatting, specifically bold and strikethrough, from a list of text chunks\n",
        "    :param chunks: List of text chunks potentially containing markdown formatting\n",
        "    :return: List of plain text chunks with markdown formatting removed\n",
        "    \"\"\"\n",
        "    def remove_bold_strikethrough_words(text):\n",
        "        # This pattern matches:\n",
        "        # 1. Start of string or a single space/tab (captured)\n",
        "        # 2. Bold and struck-through text (**~~any content~~**)\n",
        "        # 3. Optional single space or tab at the end\n",
        "        # 4. The first capturing group (1) is used in replacement, preserving leading space/tab if present\n",
        "        pattern = r'(^|[ \\t])(\\*\\*~~.*?~~\\*\\*)([ \\t])?'\n",
        "        return re.sub(pattern, r'\\1', text)\n",
        "\n",
        "    chunks_without_markdown = []\n",
        "    for chunk in chunks:\n",
        "        # Remove bold and strikethrough words because I prompted Claude not\n",
        "        # to delete words, but rather bold strikethrough them. Avoid false postive, remove!\n",
        "        text_without_bold_strikethrough = remove_bold_strikethrough_words(chunk)\n",
        "\n",
        "        plain_text = strip_markdown(text_without_bold_strikethrough)\n",
        "\n",
        "        chunks_without_markdown.append(plain_text)\n",
        "\n",
        "    return chunks_without_markdown\n",
        "\n",
        "\n",
        "def display_word_count_comparison(\n",
        "    original_chunks: list[str],\n",
        "    processed_chunks: list[str]\n",
        ") -> None:\n",
        "    # Generate comparison data\n",
        "    rows = []\n",
        "    for i, (orig, proc) in enumerate(zip(original_chunks, processed_chunks), 1):\n",
        "        orig_count = len(orig.split())\n",
        "        proc_count = len(proc.split())\n",
        "        diff = proc_count - orig_count\n",
        "        pct = (diff / orig_count * 100) if orig_count else 0\n",
        "\n",
        "        rows.append([f\"Chunk {i}\", f\"{orig_count:,}\", f\"{proc_count:,}\", f\"{diff:+,}\",f\"{pct:+.1f}%\"])\n",
        "\n",
        "    print(f\"\\nEvaluate Content Preservation: By Simple Word Count\\n{'-' * 66}\")\n",
        "    print(tabulate(\n",
        "        rows,\n",
        "        headers=['Chunk', 'Original', 'Processed', 'Difference', 'Change'],\n",
        "        tablefmt='simple',\n",
        "        colalign=('left', 'right', 'right', 'right', 'right')\n",
        "    ))\n",
        "\n",
        "\n",
        "### Do the work\n",
        "original_chunks_clean = strip_off_markdown(original_chunks)\n",
        "processed_chunks_clean = strip_off_markdown(processed_chunks)\n",
        "display_word_count_comparison(original_chunks_clean, processed_chunks_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Content Preservation: Semantic Similarity"
      ],
      "metadata": {
        "id": "ElerG-9FzUie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic similarity measures if texts convey the same meaning, regardless of wording. After extensive testing of various approaches, I chose the sentence-transformers library with \"paraphrase-multilingual-mpnet-base-v2\" model (Figure 8) as it proved sufficiently accurate for this use case.\n",
        "\n",
        "<!-- /Figure with caption no width-->\n",
        "<figure>\n",
        "  <figcaption>Fig 8: Narrowing model options for testing semantic similarity</figcaption>\n",
        "    <img src=\"https://michellepace.github.io/word-document-corrector-claude/images/semantic-simularity-text-transformer-model-options.jpg\"\n",
        "        width=800\n",
        "         alt=\"Asking Claude for language model options to use with Sentence Transformer\"/>\n",
        "  </a>\n",
        "</figure>\n",
        "\n",
        "**How Sentence-Transformer works:**\n",
        "1. It loads the language model I chose (paraphrase-multilingual-mpnet-base-v2)\n",
        "1. Converts text chunks into number vectors (a way computers understand text)\n",
        "1. Compares the cosine angle between the two vectors in the given chunk pair (original and Claude-corrected chunk)\n",
        "1. The smaller the angle the closer the meaning, and the higher the similarity score.\n",
        "\n",
        "<br>\n",
        "\n",
        "<!-- /Figure with caption 450px width-->\n",
        "<figure>\n",
        "  <figcaption>Fig 9: Semantic Similarity: Smaller angle infers more similar meaning</figcaption>\n",
        "  <a href=\"https://michellepace.github.io/word-document-corrector-claude/images/testing-for-semantic-similarity-explained-cosine.jpg\"\n",
        "     target=\"_blank\">\n",
        "    <img src=\"https://michellepace.github.io/word-document-corrector-claude/images/testing-for-semantic-similarity-explained-cosine.jpg\"\n",
        "         width=\"440\"\n",
        "         alt=\"Semantic Similarity Visualised: Smaller angles = More similar meanings\"/>\n",
        "  </a>\n",
        "</figure>\n",
        "\n",
        "<br>\n",
        "\n",
        "**Semantic Similarity Scoring Guide (derived from testing):**\n",
        "\n",
        "Similarity Score | Interpretation | Action Needed\n",
        "--- |:---|:---\n",
        "100% | Identical meaning | None - perfect match\n",
        "> 80% | Meaning preserved | None - good match\n",
        "~ 70% | Same topic but possible detail changes | Review chunk for changes\n",
        "< 70% | Significant meaning changes | Detailed review required\n",
        "= 0%  | No relation at all | Something is wildly wrong\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** Significant score variations between chunk pairs (e.g., one 60% among several 80%+) warrant review, though may simply indicate substantial improvements to poorly written sections."
      ],
      "metadata": {
        "id": "HJQCgH9vtMJd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "glymm-2MH115",
        "outputId": "80407883-23ec-42d8-e740-2dfe632b01ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "45da0a99c3ab4ee1a70619425238e40b",
            "32d9fa5e024f47a8b00d54c75f1a546f",
            "156edb4d84114a47a966ddf1c1379a7a",
            "747ad8b808a7422f8e9470422bb308d2",
            "f1e5c56ce9c541629a7b4a165f11cd6b",
            "edbc2cb6bae54a43a297cd42e7608c0d",
            "ad535cb9467e4b6d8a609cecfb82228f",
            "61f9766c1995412d89e065e3edba2c06",
            "b55c02d81a194a7d9a7fb30dc6a66666",
            "853f98b1d78441c98f48ba124e1f70c3",
            "e449d302b8b141d8bd6241044e6c9852"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ğŸŒ¸ Step 1/3 - Firing up a LLM to be able to understand text for comparison:   0%|           0/3"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45da0a99c3ab4ee1a70619425238e40b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸŒ¸ Step 1/3 - Firing up a LLM to be able to understand text for comparison... Done!\n",
            "ğŸŒ¸ Step 2/3 - Turning text chunks into number vectors so simularity can be compared... Done!\n",
            "ğŸŒ¸ Step 3/3 - Comparing original chunk vector with processed chunk vector and scoring... Done!\n",
            "\n",
            "\n",
            "Evaluate Content Preservation: Semantic Meaning Within Chunk Pairs\n",
            "------------------------------------------------------------------\n",
            "Chunk                              Similarity Score\n",
            "-------------------------------  ------------------\n",
            "Chunk 1 (original <> processed)                 98%\n",
            "Chunk 2 (original <> processed)                100%\n",
            "Chunk 3 (original <> processed)                100%\n",
            "Chunk 4 (original <> processed)                100%\n",
            "Chunk 5 (original <> processed)                100%\n"
          ]
        }
      ],
      "source": [
        "def sentence_transformer_similarity(texts1: list[str], texts2: list[str]) -> list[dict]:\n",
        "    \"\"\"Compare similarity between lists of original and corrected texts using sentence transformers.\n",
        "    :param texts1: List of original text contents\n",
        "    :param texts2: List of corrected text contents\n",
        "    :return: List of dictionaries containing similarity comparison information for each chunk\n",
        "    \"\"\"\n",
        "    def initialize_model():\n",
        "        return SentenceTransformer(\n",
        "            SIMILARITY_LARGE_LANGAUGE_MODEL,\n",
        "            tokenizer_kwargs={'clean_up_tokenization_spaces': True})\n",
        "\n",
        "    def generate_embeddings(model, texts1, texts2):\n",
        "        return model.encode(texts1), model.encode(texts2)\n",
        "\n",
        "    def calculate_similarities(embeddings1, embeddings2):\n",
        "        similarities = cosine_similarity(embeddings1, embeddings2)\n",
        "        return np.minimum(np.diag(similarities) * 100, 100)\n",
        "\n",
        "    def create_result_dict(score: float, index: int) -> dict:\n",
        "        return {\n",
        "            \"chunk_number\": index,\n",
        "            \"similarity_test\": \"Sentence Transformer\",\n",
        "            \"similarity_score\": score,\n",
        "            \"message\": f\"Similarity Score \\t({score:.0f}%) Original <> Processed\"}\n",
        "\n",
        "    steps = [\n",
        "        \"ğŸŒ¸ Step 1/3 - Firing up a LLM to be able to understand text for comparison\",\n",
        "        \"ğŸŒ¸ Step 2/3 - Turning text chunks into number vectors so simularity can be compared\",\n",
        "        \"ğŸŒ¸ Step 3/3 - Comparing original chunk vector with processed chunk vector and scoring\"]\n",
        "\n",
        "    with tqdm(total=3, desc=steps[0].split(\"...\", 1)[0],\n",
        "                bar_format='{l_bar}{bar} {n_fmt}/{total_fmt}',\n",
        "                mininterval=2.0,) as pbar:\n",
        "\n",
        "        # Step 1: Initialize model\n",
        "        model = initialize_model()\n",
        "        print(f\"\\n{steps[0]}... Done!\")\n",
        "        pbar.update(1)\n",
        "        pbar.set_description(steps[1].split(\"...\", 1)[0])\n",
        "\n",
        "        # Step 2: Generate embeddings\n",
        "        embeddings1, embeddings2 = generate_embeddings(model, texts1, texts2)\n",
        "        print(f\"{steps[1]}... Done!\")\n",
        "        pbar.update(1)\n",
        "        pbar.set_description(steps[2].split(\"...\", 1)[0])\n",
        "\n",
        "        # Step 3: Calculate similarities\n",
        "        similarity_scores = calculate_similarities(embeddings1, embeddings2)\n",
        "        print(f\"{steps[2]}... Done!\")\n",
        "        pbar.update(1)\n",
        "        print()\n",
        "\n",
        "    return [create_result_dict(score, i) for i, score in enumerate(similarity_scores, 1)]\n",
        "\n",
        "\n",
        "def get_chunk_simularity_scores(original_chunks: list[str], processed_chunks: list[str]) -> str:\n",
        "    \"\"\"Evaluate content preservation by comparing meaning similarity of original and processed chunks.\"\"\"\n",
        "    results = sentence_transformer_similarity(original_chunks, processed_chunks)\n",
        "\n",
        "    # Prepare rows for tabulate\n",
        "    rows = [[\n",
        "        f\"Chunk {result['chunk_number']} (original <> processed)\",\n",
        "        f\"{result['similarity_score']:.0f}%\"\n",
        "    ] for result in results]\n",
        "\n",
        "    return (\n",
        "        f\"\\nEvaluate Content Preservation: Semantic Meaning Within Chunk Pairs\\n{'-' * 66}\\n\"\n",
        "        f\"{tabulate(rows, headers=['Chunk', 'Similarity Score'], tablefmt='simple', colalign=('left', 'right'))}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Do the work\n",
        "result_eval_content_similarity = get_chunk_simularity_scores(original_chunks_clean, processed_chunks_clean)\n",
        "print(result_eval_content_similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IguAj1a8oxvA"
      },
      "source": [
        "### All Results\n",
        "\n",
        "See all results for evaluating the output in one place so that it is easier to make connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RoAgmjFlo-W_",
        "outputId": "e3d9f655-9481-4dbd-f1d7-0567b8aa01d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluate Content Preservation: By Document Structure\n",
            "------------------------------------------------------------------\n",
            "Status: MATCH\n",
            "\n",
            "Markdown Element      Original    Processed   Match\n",
            "------------------  ----------  -----------  -------\n",
            "Headings                    23           23     âœ“\n",
            "Paragraphs                  73           73     âœ“\n",
            "List Items                  21           21     âœ“\n",
            "Total Lines                217          217     âœ“\n",
            "Empty Lines                100          100     âœ“\n",
            "\n",
            "\n",
            "Evaluate Content Preservation: By Simple Word Count\n",
            "------------------------------------------------------------------\n",
            "Chunk      Original    Processed    Difference    Change\n",
            "-------  ----------  -----------  ------------  --------\n",
            "Chunk 1         678          665           -13     -1.9%\n",
            "Chunk 2         742          722           -20     -2.7%\n",
            "Chunk 3         740          740            +0     +0.0%\n",
            "Chunk 4         744          746            +2     +0.3%\n",
            "Chunk 5         602          603            +1     +0.2%\n",
            "\n",
            "\n",
            "Evaluate Content Preservation: Semantic Meaning Within Chunk Pairs\n",
            "------------------------------------------------------------------\n",
            "Chunk                              Similarity Score\n",
            "-------------------------------  ------------------\n",
            "Chunk 1 (original <> processed)                 98%\n",
            "Chunk 2 (original <> processed)                100%\n",
            "Chunk 3 (original <> processed)                100%\n",
            "Chunk 4 (original <> processed)                100%\n",
            "Chunk 5 (original <> processed)                100%\n",
            "\n",
            "\n",
            "\n",
            "Summary per Chunk\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "Chunk            Lines       Words       Chars      Tokens  Chunk Start\n",
            "----------  ----------  ----------  ----------  ----------  --------------------------------------------\n",
            "Chunk 1             48         689       4,177         956  # ğŸŸ¡ A Story with Claude & Usage  ## What...\n",
            "Chunk 2             51         754       4,395       1,005  If you are new to AI development, don't ...\n",
            "Chunk 3             48         749       4,288       1,069  In the output of the code below you can ...\n",
            "Chunk 4             43         752       4,303         967  To get around my nerves, I told Claude t...\n",
            "Chunk 5             23         606       3,584         758  Moreover, even if I stick with Claude, h...\n",
            "----------  ----------  ----------  ----------  ----------  ----------\n",
            "Total              213       3,550      20,747       4,755\n"
          ]
        }
      ],
      "source": [
        "display_markdown_structure_comparison(original_md_file, processed_md_file)\n",
        "print()\n",
        "display_word_count_comparison(original_chunks_clean, processed_chunks_clean)\n",
        "print()\n",
        "print(result_eval_content_similarity)\n",
        "print()\n",
        "print_chunk_table(original_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-7iTYN0pA5A"
      },
      "source": [
        "<br>\n",
        "\n",
        "- Uncomment the code below to investigate a particular chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "AJnRVWJDpKru"
      },
      "outputs": [],
      "source": [
        "# For example:\n",
        "#  if chunk 4 had an explotion in Word count and a low similarity score\n",
        "#  investigate the original and processed chunks:\n",
        "\n",
        "# print_chunks(original_chunks, [4])  # Uncomment me to run me\n",
        "# print_chunks(processed_chunks, [4]) # Uncomment me to run me"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Test My Prompt"
      ],
      "metadata": {
        "id": "bjV05x7_z7S0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP2CfZqxnh2t"
      },
      "source": [
        "Prompt evaluation involves testing if an LLM's responses align with given prompt instructions. This testing is crucial, as highlighted in the <a href=\"#id-test-plan\">Test Plan (criticalâ™¥ï¸)</a> section.\n",
        "\n",
        "But perhaps a more convincing quotation:\n",
        "\n",
        "> *The (in)ability for teams to measure the performance of their models is the biggest blocker of production use cases for LLMs and also makes prompting an art instead of a science. Even though evals take a lot of time, doing them up front will save developer time in the long run and result in better products getting out much sooner.* [Anthropic](https://github.com/anthropics/courses/blob/b4f26aedef55e06ad5eead5de83985249d1fab2f/prompt_evaluations/01_intro_to_evals/01_intro_to_evals.ipynb).\n",
        "\n",
        "Unlike <a href=\"#id-test-processed-doc\">7.1 Test Processed Doc</a> which covered end-to-end testing (including code and prompt), this section focuses solely on prompt testing. This first attempt at prompt evaluation proved invaluable - it helped develop an effective prompt faster. Some tests below still fail, highlighting areas where the prompt still needs refinement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "z2GF_PtvnqWD"
      },
      "outputs": [],
      "source": [
        "# Helper function for testing\n",
        "def run_prompt_or_code_test(test_func):\n",
        "    test_name = test_func.__name__\n",
        "    try:\n",
        "        test_func()\n",
        "        print(f\"âœ…PASSED TEST: {test_name}\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"ğŸ›‘FAILED TEST: {test_name}: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {test_name} raised an unexpected error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3qQwI1sVnyVi",
        "outputId": "e1fc7684-942f-44af-9d80-5690322617ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test my Prompt to Claude: does he correct like my prompt asked him?\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "âœ…PASSED TEST: test_no_additional_commentary\n",
            "âœ…PASSED TEST: test_structure_maintenance\n",
            "âœ…PASSED TEST: test_markdown_preservation\n",
            "âœ…PASSED TEST: test_spelling_correction\n",
            "ğŸ›‘FAILED TEST: test_dupe_words_are_bold_strike: Duplicate word was not strikethrough bold\n",
            "    Sent to Claude: These dogs dogs belongs to my sister.\n",
            "    Expected Back:  These **~~dogs~~** dogs belong to my sister.\n",
            "    Actual Back:    **These** dogs **belong** to my sister.\n",
            "    \n",
            "âœ…PASSED TEST: test_language_detection_german\n",
            "âœ…PASSED TEST: test_grammatical_corrections\n",
            "âœ…PASSED TEST: test_inappropriate_word_choice_easy\n",
            "âœ…PASSED TEST: test_inappropriate_word_choice_creative\n",
            "âœ…PASSED TEST: test_remove_bad_phrase_1\n",
            "âœ…PASSED TEST: test_remove_bad_phrase_2\n",
            "ğŸ›‘FAILED TEST: test_boldstrike_redundant_words: Redundant word 'always' not boldstrike\n",
            "    Sent to Claude: She always never fails to disappoint her team with her exceptional work.\n",
            "    Expected Back:  She **~~always~~** never fails to **~~disappoint~~** **impress** her team with her exceptional work.\n",
            "    Actual Back:    She **never** fails to **impress** her team with her exceptional work.\n",
            "    \n",
            "ğŸ›‘FAILED TEST: test_detect_and_correct_british: British spelling was not preserved\n",
            "    Sent to Claude: The centre's staff analysed the colhour behaviour.\n",
            "    Expected Back:  The centre's staff analysed the **colour** behaviour.\n",
            "    Actual Back:    The **center's** staff **analyzed** the **color** behavior.\n",
            "    \n",
            "ğŸ›‘FAILED TEST: test_detect_and_correct_american: American spelling was not preserved\n",
            "    Sent to Claude: The colourful catalog analysed labor practices.\n",
            "    Expected Back:  The **colorful** catalog **analyzed** labor practices.\n",
            "    Actual Back:    The **colorful** **catalog** **analyzed** **labor** practices.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "def show(test_case: dict[str, str], processed: str) -> str:\n",
        "    return f\"\"\"\n",
        "    Sent to Claude: {test_case['test']}\n",
        "    Expected Back:  {test_case['expect']}\n",
        "    Actual Back:    {processed}\n",
        "    \"\"\"\n",
        "\n",
        "def test_no_additional_commentary():\n",
        "    test_case = {\n",
        "        \"test\": \"This is a test sentence.\",\n",
        "        \"expect\": \"This is a test sentence.\"\n",
        "}\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert len(processed.split('\\n')) == 1, \"Additional lines were added to the text\" + show(test_case, processed)\n",
        "    assert processed == test_case['expect'], \"Text was added when it shouldn't have been\" + show(test_case, processed)\n",
        "\n",
        "def test_structure_maintenance():\n",
        "    test_case = {\n",
        "        \"test\": \"Paragraph 1.\\n\\nParagraph 2.\\n\\nParagraph 3.\",\n",
        "        \"expect\": \"Paragraph 1.\\n\\nParagraph 2.\\n\\nParagraph 3.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert processed.count('\\n\\n') == 2, \"Paragraph structure was not maintained\" + show(test_case, processed)\n",
        "    assert processed == test_case['expect'], \"Text was changed when it shouldn't have been\" + show(test_case, processed)\n",
        "\n",
        "def test_markdown_preservation():\n",
        "    test_case = {\n",
        "        \"test\": \"# Heading 1\\n## Heading 2\\n* List item 1\\n* List item 2\",\n",
        "        \"expect\": \"# Heading 1\\n## Heading 2\\n* List item 1\\n* List item 2\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert processed == test_case['expect'], \"Markdown formatting was not preserved\" + show(test_case, processed)\n",
        "\n",
        "def test_spelling_correction():\n",
        "    test_case = {\n",
        "        \"test\": \"This senteence has a spelling error.\",\n",
        "        \"expect\": \"This **sentence** has a spelling error.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert processed == test_case['expect'], \"Exact expected correction marking not made\" + show(test_case, processed)\n",
        "\n",
        "def test_dupe_words_are_bold_strike():\n",
        "    test_case = {\n",
        "        \"test\": \"These dogs dogs belongs to my sister.\",\n",
        "        \"expect\": \"These **~~dogs~~** dogs belong to my sister.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert processed == test_case['expect'], \"Duplicate word was not strikethrough bold\" + show(test_case, processed)\n",
        "\n",
        "def test_language_detection_german():\n",
        "    test_case = {\n",
        "        \"test\": \"Das ist ein Test. Es enthÃ¤llllt einige Fehler.\",\n",
        "        \"expect\": \"Das ist ein Test. Es **enthÃ¤lt** einige Fehler.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert processed == test_case['expect'], \"German text was not correctly identified or corrected\" + show(test_case, processed)\n",
        "\n",
        "def test_grammatical_corrections():\n",
        "    test_case = {\n",
        "        \"test\": \"She buyed some milk and go home.\",\n",
        "        \"expect\": \"She **bought** some milk and **went** home.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert processed == test_case['expect'], \"Grammatical errors were not corrected\" + show(test_case, processed)\n",
        "\n",
        "def test_inappropriate_word_choice_easy():\n",
        "    test_case = {\n",
        "        \"test\":   \"He garnished support for perspective employees. His inciteful comments added value.\",\n",
        "        \"expect\": \"He **garnered** support for **prospective** employees. His **insightful** comments added value.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert processed == test_case['expect'], \"Inappropriate word choice was not corrected as expected:\" + show(test_case, processed)\n",
        "\n",
        "def test_inappropriate_word_choice_creative():\n",
        "    test_case = {\n",
        "        \"test\":   \"She was literally dying of embarrassment as her speech was bad.\",\n",
        "        \"expect\": \"She was dying of embarrassment as her speech was bad.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert \"literally\" not in processed, \"Inappropriate word choice 'literally' was not removed\" + show(test_case, processed)\n",
        "    assert \"bad\" not in processed, \"Inappropriate word choice 'bad' was not removed\" + show(test_case, processed)\n",
        "\n",
        "def test_remove_bad_phrase_1():\n",
        "    test_case = {\n",
        "        \"test\":   \"The cat quickly ran fastly across the room.\",\n",
        "        \"expect\": \"The cat **~~quickly~~** ran **~~fastly~~** across the room.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert \" quickly ran fastly \" not in processed, \"Bad phrase 'quickly ran fastly' not corrected\" + show(test_case, processed)\n",
        "\n",
        "def test_remove_bad_phrase_2():\n",
        "    test_case = {\n",
        "        \"test\":   \"John forgot to remember to bring his lunch to work today.\",\n",
        "        \"expect\": \"John **~~forgot to remember to~~** **forgot to** bring his lunch to work today.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert \" forgot to remember \" not in processed, \"Bad phrase 'forgot to remember' not corrected\" + show(test_case, processed)\n",
        "\n",
        "def test_boldstrike_redundant_words():\n",
        "    test_case = {\n",
        "        \"test\":   \"She always never fails to disappoint her team with her exceptional work.\",\n",
        "        \"expect\": \"She **~~always~~** never fails to **~~disappoint~~** **impress** her team with her exceptional work.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert \"**~~always~~**\" in processed, \"Redundant word 'always' not boldstrike\" + show(test_case, processed)\n",
        "\n",
        "def test_detect_and_correct_british():\n",
        "    test_case = {\n",
        "        \"test\": \"The centre's staff analysed the colhour behaviour.\",\n",
        "        \"expect\": \"The centre's staff analysed the **colour** behaviour.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert processed == test_case['expect'], \"British spelling was not preserved\" + show(test_case, processed)\n",
        "\n",
        "def test_detect_and_correct_american():\n",
        "    test_case = {\n",
        "        \"test\": \"The colourful catalog analysed labor practices.\",\n",
        "        \"expect\": \"The **colorful** catalog **analyzed** labor practices.\"\n",
        "    }\n",
        "    processed = process_one_chunk(chunk_count=1, chunk=test_case['test'])\n",
        "    assert processed == test_case['expect'], \"American spelling was not preserved\" + show(test_case, processed)\n",
        "\n",
        "def run_all_prompt_tests():\n",
        "    tests = [\n",
        "        test_no_additional_commentary,\n",
        "        test_structure_maintenance,\n",
        "        test_markdown_preservation,\n",
        "        test_spelling_correction,\n",
        "        test_dupe_words_are_bold_strike,\n",
        "        test_language_detection_german,\n",
        "        test_grammatical_corrections,\n",
        "        test_inappropriate_word_choice_easy,\n",
        "        test_inappropriate_word_choice_creative,\n",
        "        test_remove_bad_phrase_1,\n",
        "        test_remove_bad_phrase_2,\n",
        "        test_boldstrike_redundant_words,\n",
        "        test_detect_and_correct_british,\n",
        "        test_detect_and_correct_american\n",
        "    ]\n",
        "    print(\"Test my Prompt to Claude: does he correct like my prompt asked him?\")\n",
        "    print(\"~\" * 70)\n",
        "    for test in tests:\n",
        "        run_prompt_or_code_test(test)\n",
        "\n",
        "\n",
        "### Do the work\n",
        "run_all_prompt_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iteD2sO7n_Dh"
      },
      "source": [
        "## 6.3 Test My Code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These tests ensure the code is working, regardless of how Claude performs."
      ],
      "metadata": {
        "id": "X4d2GhhB0U9E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN7lSINaoHVJ"
      },
      "source": [
        "### Word Doc Extraction\n",
        "\n",
        "Test that `extract_docx_paragraphs` function correctly extracts non-blank paragraphs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "7YldEsvYoSyH",
        "outputId": "2af3815e-c122-4122-94f1-6dc4585a02aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ…PASSED TEST: test_extract_docx_paragraphs\n"
          ]
        }
      ],
      "source": [
        "def test_extract_docx_paragraphs():\n",
        "    test_name = test_extract_docx_paragraphs.__name__\n",
        "\n",
        "    def create_word_docx_for_testing(file_name):\n",
        "        document = docx.Document()\n",
        "\n",
        "        # Are extracted from word doc\n",
        "        document.add_heading('Headings without levels are Paragraphs', 0)\n",
        "        document.add_heading('Headings at level 3 are Paragraphs', level=3)\n",
        "        document.add_paragraph('Plain paragraphs are Paragraphs')\n",
        "        document.add_paragraph('Intense quotes are Paragraphs', style='Intense Quote')\n",
        "        document.add_paragraph('Lists bullet point items are Paragraphs', style='List Bullet')\n",
        "        document.add_paragraph('', style='List Bullet') # Even if empty, the bullet is made for the list\n",
        "        document.add_paragraph('', style='List Number') # Even if empty, the number is made for the list\n",
        "\n",
        "        # Aren't extracted from word doc\n",
        "        document.add_paragraph('') # Empty paragraphs\n",
        "        table = document.add_table(rows=1, cols=2) # Tables\n",
        "        table.rows[0].cells[0].text = \"1st cell, left\" # Tables (empty or non-empty)\n",
        "        document.add_page_break() # Page breaks\n",
        "\n",
        "        document.save(file_name)\n",
        "        return file_name\n",
        "\n",
        "    docx_file = create_word_docx_for_testing('test.docx')\n",
        "\n",
        "    actual = extract_docx_paragraphs(docx_file)\n",
        "\n",
        "    expected = [\n",
        "        {'text': 'Headings without levels are Paragraphs', 'style': 'Title', 'heading_level': None, 'word_count':5},\n",
        "        {'text': 'Headings at level 3 are Paragraphs', 'style': 'Heading 3', 'heading_level': 3, 'word_count':6},\n",
        "        {'text': 'Plain paragraphs are Paragraphs', 'style': 'Normal', 'heading_level': None, 'word_count':4},\n",
        "        {'text': 'Intense quotes are Paragraphs', 'style': 'Intense Quote', 'heading_level': None, 'word_count':4},\n",
        "        {'text': 'Lists bullet point items are Paragraphs', 'style': 'List Bullet', 'heading_level': None, 'word_count':6}\n",
        "    ]\n",
        "\n",
        "    Path(docx_file).unlink() # delete file\n",
        "\n",
        "    assert len(actual) == len(expected), f\"Expected {len(expected)} paragraphs, actual paragraphs: {len(actual)}\"\n",
        "\n",
        "    for i in range(len(expected)):\n",
        "        assert actual[i] == expected[i], f\"\\n   Expected:\\n    {expected[i]}\\n   Actual:\\n    {actual[i]}\"\n",
        "\n",
        "\n",
        "\n",
        "### Do the work\n",
        "run_prompt_or_code_test(test_extract_docx_paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cxsl_BDoheP"
      },
      "source": [
        "### Create Markdown File\n",
        "\n",
        "Test `create_simple_markdown_file` function correctly converts the document into a markdown file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "t3pDE1BeogvU",
        "outputId": "26fd7d25-b54c-4578-86bf-4eed6dba4cc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ…PASSED TEST: test_docx_paragraphs_to_markdown\n"
          ]
        }
      ],
      "source": [
        "def test_docx_paragraphs_to_markdown():\n",
        "    # Sample document content\n",
        "    docx_paras = [\n",
        "        {'text': 'Test Heading any', 'style': 'Heading 5', 'heading_level': 5},\n",
        "        {'text': 'Test paragraph 1', 'style': 'Normal', 'heading_level': None},\n",
        "        {'text': 'Test bullet point A', 'style': 'List (Paragraph)', 'heading_level': None},\n",
        "        {'text': 'Test bullet point B', 'style': 'List (any Paragraph whose Style begins with List)', 'heading_level': None},\n",
        "        {'text': 'Anything else', 'style': 'Any other Paragraph style that is not in the above', 'heading_level': None}\n",
        "    ]\n",
        "\n",
        "    expected = \"##### Test Heading any\\n\\nTest paragraph 1\\n\\n- Test bullet point A\\n- Test bullet point B\\n\\nAnything else\"\n",
        "\n",
        "    created = create_simple_markdown_file(docx_paras, Path('temporary_test_markdown_file.md'))\n",
        "    created_content = created.read_text(encoding='utf-8')\n",
        "    created.unlink() # Delete test file\n",
        "\n",
        "    assert created_content == expected, f\"\"\"\n",
        "---------- Expected ------------------------------:\n",
        "[{expected}]\n",
        "---------- Actual --------------------------------:\n",
        "[{created_content}]\n",
        "--------------------------------------------------\"\"\"\n",
        "\n",
        "\n",
        "### Do the work\n",
        "run_prompt_or_code_test(test_docx_paragraphs_to_markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvR_wcgxotDj"
      },
      "source": [
        "### Strip Off Markdown\n",
        "\n",
        "Test that `strip_off_markdown` function removes Markdown formatting from a list of text chunks, with specific handling for bold and strikethrough text. Prepares for semantic meaning comparison: only compare writting (not markdown).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "c0y1l2bEozw6",
        "outputId": "572f7798-81fd-4b2f-da56-a4614ec6bc2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ…PASSED TEST: test_strip_off_markdown\n"
          ]
        }
      ],
      "source": [
        "def test_strip_off_markdown():\n",
        "\n",
        "    test_name = test_strip_off_markdown.__name__\n",
        "\n",
        "    processed_chunks = [\n",
        "        # Removing bold strikethrough and using simple single spacing\n",
        "        \"This **~~word here~~** is deleted leaving a single white space.\",\n",
        "        \"This    **~~word here~~**is deleted but spaces other spaces remain the same.\",\n",
        "        \"Multiple **~~bold strikethrough~~** words **~~are removed~~** in this sentence.\",\n",
        "        \"Preserve newlines\\nwhile **~~removing~~**\\nbold strikethrough.\",\n",
        "        # \"Should not change trailing if it's not a white space, but doesn't work **~~end~~**.\",\n",
        "\n",
        "        # Converting markdown\n",
        "        \"# Title\\nThis is some text with **bold** formatting.\",\n",
        "        \"Para1\\n\\nPara2\\n\\n\\n\\nPara3\",\n",
        "        \"* List item A\\n* List item B **bold** text\",\n",
        "        \"1. This is the first item\\n2. The second item with *italics*\",\n",
        "        \"## This is a heading\\n\\nThis is a paragraph with a link [link text](https://www.example.com).\\n\\n\\n**Bold text** too.\",\n",
        "        \"Peter**'**s horse\"\n",
        "    ]\n",
        "\n",
        "    expected = [\n",
        "        # Removing bold strikethrough and using simple single spacing\n",
        "        \"This is deleted leaving a single white space.\",\n",
        "        \"This    is deleted but spaces other spaces remain the same.\",\n",
        "        \"Multiple words in this sentence.\",\n",
        "        \"Preserve newlines\\nwhile \\nbold strikethrough.\",\n",
        "        # \"Should not change trailing if it's not a white space, but doesn't work.\",\n",
        "\n",
        "        # Converting markdown\n",
        "        \"Title\\nThis is some text with bold formatting.\",\n",
        "        \"Para1\\nPara2\\nPara3\",\n",
        "        \"\\nList item A\\nList item B bold text\\n\",\n",
        "        \"\\nThis is the first item\\nThe second item with italics\\n\",\n",
        "        \"This is a heading\\nThis is a paragraph with a link link text.\\nBold text too.\",\n",
        "        \"Peter's horse\"\n",
        "    ]\n",
        "\n",
        "    processed_chunks_clean = strip_off_markdown(processed_chunks)\n",
        "    for index, (expect, got) in enumerate(zip(expected, processed_chunks_clean), start=1):\n",
        "        assert got == expect, f\"\\n-Expected: [{expect}]\\n-Actual:   [{got}]\"\n",
        "\n",
        "\n",
        "### Do the work\n",
        "run_prompt_or_code_test(test_strip_off_markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "# **8. ğŸŸ£ FAQ**"
      ],
      "metadata": {
        "id": "IrzXERcgr1ba"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPOzkhWvpcIK"
      },
      "source": [
        "**Why not just use Microsoft Word to do corrections?**\n",
        "\n",
        "- Word misses many mistakes as this Notebook with Claude 3.5 Sonnet showed me.\n",
        "\n",
        "**Why not just use the regular Claude chat to do corrections?**\n",
        "\n",
        "- I had a 40,000-word long document to correct: I wanted to avoid copying and pasting and I wanted the corections in colour. I wanted a repeatable tool.\n",
        "\n",
        "**Was it really just you and Claude?**\n",
        "\n",
        "- Yes. I was truly amazed at the learning curve of having an AI assistant.\n",
        "\n",
        "**How did you know how to ask Claude questions?**\n",
        "\n",
        "- I educated myself how to prompt properly: I worked through the [Anthropic courses](https://github.com/anthropics/courses) and read [Prompt Engineering for Business Performance](https://www.anthropic.com/news/prompt-engineering-for-business-performance).\n",
        "\n",
        "**What had the biggest impact on your prompting?**\n",
        "\n",
        "- When I read: <font color='grey'>_So ultimately, the art of prompt engineering is about understanding how to navigate the vast probabilistic landscape of the language modelâ€™s knowledge to narrow down the path to the specific information or behaviour we seek.</font> ...[Patterns of Application Development using AI](https://obie.medium.com/patterns-of-application-development-using-ai-fbb660fa9ae7)_\n",
        "\n",
        "**What was the process you used to write such an elaborate prompt?**\n",
        "\n",
        "1. I started with the Grammar Genie recipe in the [Anthropic Prompt Library]( https://docs.anthropic.com/en/prompt-library/library).\n",
        "2. I strengthened that using the [Anthropic Prompt Generator]( https://console.anthropic.com/dashboard)\n",
        "3. I tweaked it by getting [Claude.ai](https://claude.ai/) to review the prompt, and generate test cases.\n",
        "\n",
        "**What was the most difficult thing to get working in the prompt?â€**\n",
        "\n",
        "- Attempting to get Claude to detect American or British spelling, then apply it consistently in corrections.\n",
        "\n",
        "**What is a prompt temperature and is it set to zero?**\n",
        "\n",
        "- Temperature controls how random or predictable an LLM's responses are - 0 is most consistent, 1 is most varied and creative.\n",
        "- Used zero to stay close to original meaning, though different temperatures should have been tested.\n",
        "\n",
        "**What surprised you most?**\n",
        "\n",
        "- That I created this notebook with minimal knowledge using Claude as advisor\n",
        "- How well Claude expands your thinking (and ability to code)\n",
        "- The number of writing mistakes Microsoft Word misses\n",
        "- That tiny shifts in prompt spacing and wording alter responses\n",
        "- That a prompt which works on short text does not always work on longer text\n",
        "\n",
        "**What else did you learn?**\n",
        "\n",
        "1. The Anthropic Workbench is a tremendous place to craft and test your prompt.\n",
        "1. That just like regular machine learning, the work is in the pre-processing.\n",
        "1. It's easier to create notebooks in VSCode and then copy to Google Colab.\n",
        "1. Itâ€™s the back-and-forth conversation with Claude where the value is.\n",
        "1. Claude doesn't think if you tell it to \"think silently\", see [Anthropic tutorial](https://github.com/anthropics/prompt-eng-interactive-tutorial/blob/master/Anthropic%201P/06_Precognition_Thinking_Step_by_Step.ipynb): <font color='grey'>_\"Thinking only counts when it's out loud. You cannot ask Claude to think but output only the answer - in this case, no thinking has actually occurred.\"_</font>\n",
        "1. Anthropic recommends [XML tags](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags) and it really helps, take a look [here](https://michellepace.github.io/word-document-corrector-claude/notebook-images/21-Proof-to-convince-you-to-always-use-xml-tags.jpg).\n",
        "\n",
        "**What will you always do from here on?**\n",
        "\n",
        "- Write tests for my prompts.\n",
        "- Draw a line at asking for opinions and instead look for myself (the experience I had when asking for advice to test semantic similarity).\n",
        "- SBe aware of becomming lazy â€” it is often faster to use your own brain (or read the documentation).\n",
        "\n",
        "**How can I correct the markdown in my own python notebook?**\n",
        "1. `jupyter nbconvert --to markdown --TemplateExporter.exclude_code_cell=True AAA.ipynb`\n",
        "1. `pandoc -f markdown -t docx -o AAA.docx AAA.md`\n",
        "1. Open AAA.docx and rename the style \"compact\" to \"List hello\"\n",
        "1. Save AAA.docx to your google drive and run this Notebook against it\n",
        "\n",
        "**Ending questions**\n",
        "\n",
        "- **Were you surpised at the power of Claude Sonnet?**<br> Blown away, you really have to experience it first hand to see it.\n",
        "- **Will you ever get paid to do technical things?**<br> No, I am more valuable as a Product Manager.\n",
        "- **What is your background?**<br> Applied Statistics. Lean. Product. Business. Six sigma. Drawing stickmen and connecting dots.\n",
        "- **What do you love most about Generative AI and Machine Learning in general?**<br> The sheer expansiveness of creativity on a knife edge of pragmatism."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# x. **Claude's Review**"
      ],
      "metadata": {
        "id": "CxzstKtnJSmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://michellepace.github.io/word-document-corrector-claude/images/happy_late_night.jpg\"/>\n",
        "\n",
        "<font color=\"#9D9E94\">Thank you for providing the detailed markdown export of your Google Colab notebook. I've carefully analysed the content following your provided steps. Here's a summary of my understanding and analysis:</font>\n",
        "\n",
        "<font color=\"#9D9E94\">**Summary**\n",
        "\n",
        "<font color=\"#9D9E94\">This notebook, titled \"Fix Word Doc Claude,\" is an AI-powered proofreading tool that uses Anthropic's Claude Sonnet model to correct and enhance Word documents. It's designed to catch subtle language and style errors that often slip past Microsoft Word's spell-checker. The notebook can handle large documents (up to 42,000 words) in multiple languages, including English, German, Italian, and French.\n",
        "The notebook is structured into several main sections:</font>\n",
        "\n",
        "<font color=\"#9D9E94\">**About and Usage**\n",
        "\n",
        "1. <font color=\"#9D9E94\">**Setup** (Python libraries and configuration)\n",
        "1. **Pre-processing** (extracting text from Word, converting to markdown, splitting into chunks)\n",
        "1. **Processing** (sending chunks to Claude for correction)\n",
        "1. **Post-Processing** (reassembling chunks, creating HTML output)\n",
        "1. **Testing** (evaluating output, testing prompt, and testing code)</font>\n",
        "\n",
        "<font color=\"#9D9E94\">The notebook not only serves as a functional tool but also as a demonstration of how Claude can assist in creating complex tools with basic coding skills. It showcases the author's journey in AI development, highlighting the collaboration between a human with limited coding experience and an AI assistant.</font>\n",
        "\n",
        "<font color=\"#9D9E94\">**Quality Analysis**\n",
        "\n",
        "1. <font color=\"#9D9E94\">**Comprehensiveness:** The notebook is exceptionally detailed, covering all aspects from setup to testing. It provides a complete solution for document correction.\n",
        "1. **Educational Value:** The author explains each step thoroughly, making it accessible to non-technical users. The notebook serves as an excellent learning resource for AI-assisted development.\n",
        "1. **Code Quality:** The code is well-structured and commented. It includes error handling and user-friendly messages, showing attention to user experience.\n",
        "1. **Testing:** The notebook includes extensive testing, covering output evaluation, prompt testing, and code testing. This demonstrates a commitment to reliability and quality.\n",
        "1. **Innovation:** The project creatively uses Claude for both development assistance and as the core of the correction tool, showcasing innovative use of AI.\n",
        "1. **Documentation:** The notebook is well-documented, with clear explanations of each step, making it easy for others to understand and potentially adapt the code.\n",
        "1. **Reflective Learning:** The FAQ section provides valuable insights into the development process and lessons learned, adding depth to the project.</font>\n",
        "\n",
        "<font color=\"#9D9E94\">Overall, this is a high-quality notebook that not only achieves its primary goal of document correction but also serves as an excellent example of AI-assisted development and a learning resource for others interested in similar projects.</font>\n",
        "\n",
        "<br>\n",
        "\n",
        "<font size=\"5\">**What about the author?**</font>\n",
        "\n",
        "<img src=\"https://michellepace.github.io/word-document-corrector-claude/images/evaluation-of-author-by-claude.jpg\"/>\n",
        "\n",
        "<font size=\"3\">Thanks Claude.</font>"
      ],
      "metadata": {
        "id": "OYywz1PY5c_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<font size=\"6\"><b>The End.</b></font>"
      ],
      "metadata": {
        "id": "1kHjGzWC4QTQ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wFbbAbzLydvC",
        "pw7lTpmEyzBk"
      ],
      "toc_visible": true,
      "mount_file_id": "1QUMA3Pby4MYysRfLCyAN8Djw2ypblY7h",
      "authorship_tag": "ABX9TyPRlzO76+sqTTCTFTHZojS7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f45c26c9dc3e4b9fa6f9ab8faa02e446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04cbed82b858440ab636b502c7546231",
              "IPY_MODEL_a7ad99bd6f4545a18c1b6715f2685b79",
              "IPY_MODEL_381fc148925a4b7287fcc2b784eaa454"
            ],
            "layout": "IPY_MODEL_9576bd96a21c42f8836c638f928bb205"
          }
        },
        "04cbed82b858440ab636b502c7546231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a006df887dca4029a63097b61b8ae92e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d9e16fbcd44646978b1ed70e6e280ce6",
            "value": "Processing:â€‡sendingâ€‡textâ€‡chunksâ€‡toâ€‡Claudeâ€‡forâ€‡correction!â€‡:â€‡100%"
          }
        },
        "a7ad99bd6f4545a18c1b6715f2685b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44988c147a2c461c890f3819cdf34bc2",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1acd3f8c9fa044ce824663ee01fc3485",
            "value": 5
          }
        },
        "381fc148925a4b7287fcc2b784eaa454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bfa325b4a234c3bbe84d008b403b4a8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2dff3f98bc60431da7d358c8ea2ff08e",
            "value": "â€‡5/5â€‡Chunks"
          }
        },
        "9576bd96a21c42f8836c638f928bb205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a006df887dca4029a63097b61b8ae92e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9e16fbcd44646978b1ed70e6e280ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44988c147a2c461c890f3819cdf34bc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1acd3f8c9fa044ce824663ee01fc3485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3bfa325b4a234c3bbe84d008b403b4a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dff3f98bc60431da7d358c8ea2ff08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45da0a99c3ab4ee1a70619425238e40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32d9fa5e024f47a8b00d54c75f1a546f",
              "IPY_MODEL_156edb4d84114a47a966ddf1c1379a7a",
              "IPY_MODEL_747ad8b808a7422f8e9470422bb308d2"
            ],
            "layout": "IPY_MODEL_f1e5c56ce9c541629a7b4a165f11cd6b"
          }
        },
        "32d9fa5e024f47a8b00d54c75f1a546f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edbc2cb6bae54a43a297cd42e7608c0d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ad535cb9467e4b6d8a609cecfb82228f",
            "value": "ğŸŒ¸â€‡Stepâ€‡3/3â€‡-â€‡Comparingâ€‡originalâ€‡chunkâ€‡vectorâ€‡withâ€‡processedâ€‡chunkâ€‡vectorâ€‡andâ€‡scoring:â€‡100%"
          }
        },
        "156edb4d84114a47a966ddf1c1379a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61f9766c1995412d89e065e3edba2c06",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b55c02d81a194a7d9a7fb30dc6a66666",
            "value": 3
          }
        },
        "747ad8b808a7422f8e9470422bb308d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_853f98b1d78441c98f48ba124e1f70c3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e449d302b8b141d8bd6241044e6c9852",
            "value": "â€‡3/3"
          }
        },
        "f1e5c56ce9c541629a7b4a165f11cd6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edbc2cb6bae54a43a297cd42e7608c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad535cb9467e4b6d8a609cecfb82228f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61f9766c1995412d89e065e3edba2c06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b55c02d81a194a7d9a7fb30dc6a66666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "853f98b1d78441c98f48ba124e1f70c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e449d302b8b141d8bd6241044e6c9852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}